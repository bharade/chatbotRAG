{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain langchain_community tiktoken langchain-nomic \"nomic[local]\" langchain-ollama scikit-learn langgraph tavily-python bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling aabd4debf0c8... 100% ▕████████████████▏ 1.1 GB                         \n",
      "pulling 369ca498f347... 100% ▕████████████████▏  387 B                         \n",
      "pulling 6e4c38e1172f... 100% ▕████████████████▏ 1.1 KB                         \n",
      "pulling f4d24e9138dd... 100% ▕████████████████▏  148 B                         \n",
      "pulling a85fe2a2e58e... 100% ▕████████████████▏  487 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the Model *deepseek-r1:1.5b***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "local_llm = \"deepseek-r1:1.5b\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for Loading the JSON file and created VectorBase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts: 100%|██████████| 16/16 [00:13<00:00,  1.20inputs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store reloaded from JSON!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"vectorstore.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vector_data = json.load(f)\n",
    "\n",
    "# Reconstruct the vector store\n",
    "vectorstore = SKLearnVectorStore.from_texts(\n",
    "    texts=vector_data[\"documents\"],\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    "    metadatas=vector_data[\"metadata\"],\n",
    ")\n",
    "\n",
    "print(\"Vector store reloaded from JSON!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   0%|          | 0/3921 [00:00<?, ?inputs/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m doc_splits \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(docs_list)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Step 3: Add to vectorDB using embeddings\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mSKLearnVectorStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNomicEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnomic-embed-text-v1.5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Step 4: Create retriever (ensure k does not exceed available documents)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mlen\u001b[39m(doc_splits)))\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive\\Documents\\RAG_project\\myenv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:843\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[0;32m    841\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m--> 843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive\\Documents\\RAG_project\\myenv\\Lib\\site-packages\\langchain_community\\vectorstores\\sklearn.py:354\u001b[0m, in \u001b[0;36mSKLearnVectorStore.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, persist_path, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    352\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSKLearnVectorStore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    353\u001b[0m     vs \u001b[38;5;241m=\u001b[39m SKLearnVectorStore(embedding, persist_path\u001b[38;5;241m=\u001b[39mpersist_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 354\u001b[0m     \u001b[43mvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vs\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive\\Documents\\RAG_project\\myenv\\Lib\\site-packages\\langchain_community\\vectorstores\\sklearn.py:207\u001b[0m, in \u001b[0;36mSKLearnVectorStore.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m _ids \u001b[38;5;241m=\u001b[39m ids \u001b[38;5;129;01mor\u001b[39;00m [\u001b[38;5;28mstr\u001b[39m(uuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m _texts]\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_texts\u001b[38;5;241m.\u001b[39mextend(_texts)\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embeddings\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_texts\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadatas\u001b[38;5;241m.\u001b[39mextend(metadatas \u001b[38;5;129;01mor\u001b[39;00m ([{}] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(_texts)))\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ids\u001b[38;5;241m.\u001b[39mextend(_ids)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive\\Documents\\RAG_project\\myenv\\Lib\\site-packages\\langchain_nomic\\embeddings.py:114\u001b[0m, in \u001b[0;36mNomicEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[0;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed search docs.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m        texts: list of texts to embed as documents\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_document\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive\\Documents\\RAG_project\\myenv\\Lib\\site-packages\\langchain_nomic\\embeddings.py:98\u001b[0m, in \u001b[0;36mNomicEmbeddings.embed\u001b[1;34m(self, texts, task_type)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21membed\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39m, task_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[0;32m     90\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed texts.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m            `search_document`, `classification`, `clustering`\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43membed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdimensionality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimensionality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive\\Documents\\RAG_project\\myenv\\Lib\\site-packages\\nomic\\embed.py:193\u001b[0m, in \u001b[0;36mtext\u001b[1;34m(texts, model, task_type, dimensionality, long_text_mode, inference_mode, device, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_embed4all:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_text_embed4all\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdimensionality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlong_text_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdynamic_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m CancellationError:\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# dynamic mode chose to use Atlas, fall through\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive\\Documents\\RAG_project\\myenv\\Lib\\site-packages\\nomic\\embed.py:297\u001b[0m, in \u001b[0;36m_text_embed4all\u001b[1;34m(texts, model, task_type, dimensionality, long_text_mode, dynamic_mode, **kwargs)\u001b[0m\n\u001b[0;32m    295\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(texts), start \u001b[38;5;241m+\u001b[39m batch_size)\n\u001b[0;32m    296\u001b[0m b \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m--> 297\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43m_embed4all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimensionality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdimensionality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlong_text_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlong_text_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43matlas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcancel_cb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancel_cb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdynamic_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m ntok \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_prompt_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    307\u001b[0m output_embeddings\u001b[38;5;241m.\u001b[39mextend(out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive\\Documents\\RAG_project\\myenv\\Lib\\site-packages\\gpt4all\\gpt4all.py:159\u001b[0m, in \u001b[0;36mEmbed4All.embed\u001b[1;34m(self, text, prefix, dimensionality, long_text_mode, return_dict, atlas, cancel_cb)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLong text mode must be one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncate\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlong_text_mode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 159\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt4all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensionality\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matlas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancel_cb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive\\Documents\\RAG_project\\myenv\\Lib\\site-packages\\gpt4all\\_pyllmodel.py:445\u001b[0m, in \u001b[0;36mLLModel.generate_embeddings\u001b[1;34m(self, text, prefix, dimensionality, do_mean, atlas, cancel_cb)\u001b[0m\n\u001b[0;32m    442\u001b[0m cancel_cb_wrapper \u001b[38;5;241m=\u001b[39m EmbCancelCallback() \u001b[38;5;28;01mif\u001b[39;00m cancel_cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m EmbCancelCallback(wrap_cancel_cb)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# generate the embeddings\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m embedding_ptr \u001b[38;5;241m=\u001b[39m \u001b[43mllmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllmodel_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensionality\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_count\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matlas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancel_cb_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m embedding_ptr:\n\u001b[0;32m    451\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(unknown error)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m error\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mdecode()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "from langchain.schema import Document  # Import Document class\n",
    "\n",
    "# Get file list for JSON files\n",
    "json_files = [f for f in os.listdir() if f.endswith(\".json\")]\n",
    "docs_list = []\n",
    "\n",
    "# Load JSON files\n",
    "for json_path in json_files:\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:  # Use UTF-8 encoding\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Assuming JSON is a list of page data\n",
    "            if isinstance(data, list):\n",
    "                for page in data:\n",
    "                    title = page.get('title', '')\n",
    "                    content = page.get('content', '')\n",
    "                    url = page.get('url', '')\n",
    "                    timestamp = page.get('timestamp', '')\n",
    "                    \n",
    "                    # Create Document object with content and metadata\n",
    "                    document = Document(\n",
    "                        page_content=content,\n",
    "                        metadata={\"title\": title, \"url\": url, \"timestamp\": timestamp}\n",
    "                    )\n",
    "                    docs_list.append(document)\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Error decoding {json_path}: {e}\")\n",
    "\n",
    "# Step 2: Split documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1024, chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Step 3: Add to vectorDB using embeddings\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    ")\n",
    "\n",
    "# Step 4: Create retriever (ensure k does not exceed available documents)\n",
    "retriever = vectorstore.as_retriever(k=min(2, len(doc_splits)))\n",
    "\n",
    "print(f\"Loaded {len(doc_splits)} document chunks from {len(json_files)} JSON files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Vectorbase from the Scraped Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading Academic Units Academic Units Acade.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive\\Documents\\RAG_project\\myenv\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py:43\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 43\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 3379: character maps to <undefined>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m txt_path \u001b[38;5;129;01min\u001b[39;00m txt_files:\n\u001b[0;32m     19\u001b[0m     loader \u001b[38;5;241m=\u001b[39m TextLoader(txt_path)\n\u001b[1;32m---> 20\u001b[0m     docs_list\u001b[38;5;241m.\u001b[39mextend(\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# # Step 1: Load all PDFs in the current directory\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# pdf_files = [f for f in os.listdir() if f.endswith(\".pdf\")]\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# docs_list = []\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Step 2: Split documents\u001b[39;00m\n\u001b[0;32m     32\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter\u001b[38;5;241m.\u001b[39mfrom_tiktoken_encoder(\n\u001b[0;32m     33\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[0;32m     34\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive\\Documents\\RAG_project\\myenv\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:31\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive\\Documents\\RAG_project\\myenv\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py:56\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error loading Academic Units Academic Units Acade.txt"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader  # Load PDFs\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "\n",
    "pdf_files = [f for f in os.listdir() if f.endswith(\".pdf\")]\n",
    "txt_files = [f for f in os.listdir() if f.endswith(\".txt\")]\n",
    "docs_list = []\n",
    "\n",
    "# Load PDFs\n",
    "# for pdf_path in pdf_files:\n",
    "#     loader = PyMuPDFLoader(pdf_path)\n",
    "#     docs_list.extend(loader.load())  # Append loaded documents from PDFs\n",
    "\n",
    "# Load TXT files\n",
    "for txt_path in txt_files:\n",
    "    loader = TextLoader(txt_path)\n",
    "    docs_list.extend(loader.load())\n",
    "\n",
    "\n",
    "# # Step 1: Load all PDFs in the current directory\n",
    "# pdf_files = [f for f in os.listdir() if f.endswith(\".pdf\")]\n",
    "# docs_list = []\n",
    "\n",
    "# for pdf_path in pdf_files:\n",
    "#     loader = PyMuPDFLoader(pdf_path)\n",
    "#     docs_list.extend(loader.load())  # Append loaded documents\n",
    "\n",
    "# Step 2: Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1024, chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Step 3: Add to vectorDB\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    ")\n",
    "\n",
    "# Step 4: Create retriever (ensure k does not exceed available documents)\n",
    "retriever = vectorstore.as_retriever(k=min(2, len(doc_splits)))\n",
    "\n",
    "print(f\"Loaded {len(doc_splits)} document chunks from {len(pdf_files)} PDFs.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts: 100%|██████████| 15/15 [00:28<00:00,  1.91s/inputs]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 document chunks from 2 TXT files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "#from langchain.document_loaders import Document\n",
    "\n",
    "\n",
    "txt_files = [f for f in os.listdir() if f.endswith(\".txt\")]\n",
    "docs_list = []\n",
    "\n",
    "# Load TXT files with custom encoding handling\n",
    "for txt_path in txt_files:\n",
    "    try:\n",
    "        with open(txt_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            content = file.read()\n",
    "            docs_list.append(Document(page_content=content, metadata={\"source\": txt_path}))\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Error loading {txt_path} due to encoding issues.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading {txt_path}: {e}\")\n",
    "\n",
    "# Step 2: Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1024, chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Step 3: Add to vectorDB\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    ")\n",
    "\n",
    "# Step 4: Create retriever (ensure k does not exceed available documents)\n",
    "retriever = vectorstore.as_retriever(k=min(2, len(doc_splits)))\n",
    "\n",
    "print(f\"Loaded {len(doc_splits)} document chunks from {len(txt_files)} TXT files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorbase created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tavily API integration*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set API key from environment or prompt user\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Enter value for {var}: \")\n",
    "\n",
    "# Ensure API key is set\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Set parallelism setting for tokenizers\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Prompt\n",
    "# router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "\n",
    "# The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "\n",
    "# Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n",
    "\n",
    "# Return JSON with single key, datasource, that is 'websearch' or 'vectorstore' depending on the question.\"\"\"\n",
    "\n",
    "router_instructions = \"\"\"\n",
    "You are an expert router for directing user queries.\n",
    "\n",
    "The vectorstore contains documents related to:\n",
    "- job role\n",
    "- person qualification required\n",
    "- skills\n",
    "\n",
    "Use 'vectorstore' if the query is related to any of these topics.\n",
    "For everything else (including current events), return 'websearch'.\n",
    "\n",
    "### IMPORTANT ###\n",
    "Your response MUST be a **valid JSON object** with exactly one key:\n",
    "{\n",
    "  \"datasource\": \"vectorstore\"\n",
    "}\n",
    "or\n",
    "{\n",
    "  \"datasource\": \"websearch\"\n",
    "}\n",
    "\n",
    "Do NOT include explanations, additional keys, or any other information.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Test router\n",
    "test_web_search = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions)]\n",
    "    + [\n",
    "        HumanMessage(\n",
    "            content=\"What are the required skills for this job role?\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "# test_web_search_2 = llm_json_mode.invoke(\n",
    "#     [SystemMessage(content=router_instructions)]\n",
    "#     + [HumanMessage(content=\"What are the models released today for llama3.2?\")]\n",
    "# )\n",
    "# test_vector_store = llm_json_mode.invoke(\n",
    "#     [SystemMessage(content=router_instructions)]\n",
    "#     + [HumanMessage(content=\"What are the types of agent memory?\")]\n",
    "# )\n",
    "print(\n",
    "    json.loads(test_web_search.content)\n",
    "    # json.loads(test_web_search_2.content),\n",
    "    # json.loads(test_vector_store.content),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assessing the goodness of Retrieved Document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts: 100%|██████████| 1/1 [00:00<00:00, 12.00inputs/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'key': 'relevant', 'score': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "# Doc grader instructions\n",
    "doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
    "\n",
    "# Test\n",
    "question = \"What is the job description for tescra ACHNET company?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "doc_grader_prompt_formatted = doc_grader_prompt.format(\n",
    "    document=doc_txt, question=question\n",
    ")\n",
    "result = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=doc_grader_instructions)]\n",
    "    + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    ")\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integration and routing of \"VectorBase\" and \"Web Search\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts:   0%|          | 0/1 [00:00<?, ?inputs/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts: 100%|██████████| 1/1 [00:00<00:00,  7.89inputs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt: Here is the retrieved document: \n",
      "\n",
      " Detailed Job Description \n",
      "Job Profile \n",
      "• Job Designation: Software Engineer \n",
      "• Place of Posting: Bangalore/ Remote \n",
      "ACHNET, the world's leading AI-driven marketplace that fosters professional growth and \n",
      "collaboration, and is made with love by IITians, also we have been loved by IITians as we have been \n",
      "hiring them with the utmost preference in building our employee ecosystems.  \n",
      "We are seeking skilled and motivated Software Development Engineers (SDEs) to join our agile \n",
      "team and contribute to the development of robust and feature-rich web applications. As an SDE at \n",
      "ACHNET, you will be a crucial part of our mission to create exceptional user experiences and drive \n",
      "innovation in technology. \n",
      "Role and Responsibilities:  \n",
      "As an SDE focusing on web application development, you will be responsible for the entire software \n",
      "development lifecycle, from requirements analysis to deployment. You will work collaboratively \n",
      "within an agile team, often as an individual contributor, to create, enhance, and maintain various \n",
      "modules that constitute our expansive product platform. Your key responsibilities will include: \n",
      " \n",
      "Collaborating with cross-functional teams to understand design specifications and \n",
      "requirements for web applications. \n",
      " \n",
      "Developing web applications using a technical stack that includes Microsoft Technologies, \n",
      "HTML, CSS, JavaScript, MongoDB, Redis, Python and other relevant open-source \n",
      "frameworks. \n",
      " \n",
      "Conducting thorough requirements analysis, application development, documentation, and \n",
      "testing within an agile development environment. \n",
      " \n",
      "Designing, coding, and debugging applications and systems using a variety of software tools \n",
      "and programming languages. \n",
      " \n",
      "Ensuring the performance, security, and scalability of developed applications. \n",
      "Required Skills and Qualifications:  \n",
      "To excel in this role, you should possess the following skills and qualifications: \n",
      " \n",
      "Strong understanding of data structures and their application in software development. \n",
      " \n",
      "Knowledge of Machine Learning Models like YOLO, LLM, NER etc. is good to have. \n",
      " \n",
      "Experience or exposure to developing object-oriented programming (OOP) solutions. \n",
      " \n",
      "Proficiency in styling front-end web pages using CSS, HTML, and JavaScript based on design \n",
      "specifications. \n",
      " \n",
      "Knowledge or experience in interfacing applications with NoSQL databases like Redis or \n",
      "MongoDB is a plus.\n",
      "\n",
      "Qualifications and Experience: \n",
      " \n",
      "Bachelor's degree in Computer Science, Software Engineering, or a related field (Master's \n",
      "degree preferred). \n",
      " \n",
      "Proven experience in web application development and a strong portfolio showcasing your \n",
      "projects. \n",
      " \n",
      "Proficiency in relevant programming languages, tools, and technologies. \n",
      " \n",
      "Prior experience working in an agile development environment is highly beneficial.\n",
      "\n",
      "Transformation – it’s the climb\n",
      "Demonstrated Passion for Red Teaming\n",
      "•\n",
      "Bug Bounty and Hackathons Participation\n",
      "•\n",
      "Certifications: CEH, EJPT\n",
      "•\n",
      " TryHackMe (THM), HackTheBox (HTB)\n",
      "•\n",
      "Hacker1 Initiatives\n",
      "•\n",
      "Financial Status Meter: $ \n",
      "•\n",
      "Advanced Certifications: OSCP, CRTP, CISSP, \n",
      "CEH Master\n",
      "•\n",
      "Strong LinkedIn Network in Cybersecurity\n",
      "•\n",
      "Industry Recognition: Sought-After Speaker, \n",
      "Aspiring Red Teamer\n",
      "•\n",
      "Memberships: OWASP, IS2, EC Council, AI \n",
      "Exchange\n",
      "•\n",
      "Skills: Seasoned and Skilled Professionals\n",
      "•\n",
      "Financial Status Meter: $$$ \n",
      "Year 1 – Building Foundations\n",
      "Year 3 – Scaling New Heights\n",
      "\n",
      "02\n",
      "Learning and growth\n",
      "Industry Exposure:\n",
      "Work on projects involving cutting-edge \n",
      "technologies like Red Teaming for AI models and \n",
      "Generative AI risk assessments.\n",
      "Mentorship:\n",
      "Learn directly from industry leaders who’ve worked \n",
      "at companies like Microsoft, Cisco, and Palo Alto \n",
      "Networks.\n",
      "01\n",
      "Certifications:\n",
      "We support you in earning certifications like CISSP, \n",
      "CEH, and ISO Lead Auditor credentials.\n",
      "03 \n",
      "\n",
      " Here is the user question: \n",
      "\n",
      " what is the qualification requirement for tescra achnet software role?. \n",
      "\n",
      "    Carefully and objectively assess whether the document contains **specific information** that is directly relevant to the question. \n",
      "\n",
      "    ### IMPORTANT ###\n",
      "    - The document must explicitly mention or provide information related to the question.\n",
      "    - If the document does not contain any information related to the question, return \"No\".\n",
      "    - Your response MUST be a **valid JSON object** with exactly one key:\n",
      "    {\n",
      "      \"binary_score\": \"Yes\"\n",
      "    }\n",
      "    or\n",
      "    {\n",
      "      \"binary_score\": \"No\"\n",
      "    }\n",
      "\n",
      "    Do NOT include explanations, additional keys, or any other information.\n",
      "    \n",
      "Raw Grading Response: {\n",
      "\n",
      "}\n",
      "\n",
      " \n",
      "     \n",
      "                \n",
      "   \n",
      "                \n",
      "   \n",
      "                \n",
      "   \n",
      "                \n",
      "    \n",
      "                \n",
      "   \n",
      "                \n",
      "   \n",
      "                \n",
      "   \n",
      "                \n",
      "   \n",
      "                \n",
      "  \n",
      "Parsed JSON: {}\n",
      "Invalid JSON structure or missing 'binary_score' key.\n",
      "No relevant context found. Searching the web using TAVIly...\n",
      "Error: 404 - {\"detail\":\"Not Found\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import json\n",
    "\n",
    "\n",
    "# RAG Prompt\n",
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "\n",
    "Here is the context to use to answer the question:\n",
    "\n",
    "{context}\n",
    "\n",
    "Think carefully about the above context.\n",
    "\n",
    "Now, review the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Provide an answer to this question using only the above context.\n",
    "\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Function to search using TAVIly API\n",
    "def tavili_search(query):\n",
    "    url = \"https://api.tavily.com/v1/search\"  # Replace with TAVIly's actual search endpoint\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('TAVILY_API_KEY')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"num_results\": 5  # You can adjust the number of results you want\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        return results['data']  # Assuming the response returns a 'data' key containing search results\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\"\n",
    "\n",
    "\n",
    "# Function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Function to check if retrieved docs are relevant\n",
    "# def is_relevant(docs, question):\n",
    "#     doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "#     Carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "#     ### IMPORTANT ###\n",
    "#     Your response MUST be a **valid JSON object** with exactly one key:\n",
    "#     {{\n",
    "#       \"binary_score\": \"Yes\"\n",
    "#     }}\n",
    "#     or\n",
    "#     {{\n",
    "#       \"binary_score\": \"No\"\n",
    "#     }}\n",
    "\n",
    "#     Do NOT include explanations, additional keys, or any other information.\n",
    "#     \"\"\"\n",
    "#     #doc_grader_prompt = \"\"\"Here is the document retrieved from your search: \\n\\n{document}\\n\\nThe user has asked the following question: \\n\\n{question}\\n\\nDoes this document contain relevant information to answer the question? Please respond with either 'Yes' or 'No'.\"\"\"\n",
    "\n",
    "\n",
    "#     docs_txt = format_docs(docs)\n",
    "#     doc_grader_prompt_formatted = doc_grader_prompt.format(document=docs_txt, question=question)\n",
    "\n",
    "#     grading_response = llm_json_mode.invoke(\n",
    "#         [SystemMessage(content=\"You are a grader assessing relevance of a retrieved document.\")]\n",
    "#         + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    "#     )\n",
    "#     print(f\"Grading response: {grading_response.content}\")\n",
    "#     try:\n",
    "#         grading_result = json.loads(grading_response.content)\n",
    "#         return grading_result.get(\"binary_score\", \"No\") == \"Yes\"\n",
    "#     except json.JSONDecodeError:\n",
    "#         return False  # If the model fails to return valid JSON, assume irrelevance.\n",
    "\n",
    "def is_relevant(docs, question):\n",
    "    doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "    Carefully and objectively assess whether the document contains **specific information** that is directly relevant to the question. \n",
    "\n",
    "    ### IMPORTANT ###\n",
    "    - The document must explicitly mention or provide information related to the question.\n",
    "    - If the document does not contain any information related to the question, return \"No\".\n",
    "    - Your response MUST be a **valid JSON object** with exactly one key:\n",
    "    {{\n",
    "      \"binary_score\": \"Yes\"\n",
    "    }}\n",
    "    or\n",
    "    {{\n",
    "      \"binary_score\": \"No\"\n",
    "    }}\n",
    "\n",
    "    Do NOT include explanations, additional keys, or any other information.\n",
    "    \"\"\"\n",
    "\n",
    "    docs_txt = format_docs(docs)\n",
    "    doc_grader_prompt_formatted = doc_grader_prompt.format(document=docs_txt, question=question)\n",
    "\n",
    "    # Debug: Print the formatted prompt\n",
    "    print(\"Formatted Prompt:\", doc_grader_prompt_formatted)\n",
    "\n",
    "    grading_response = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=\"You are a grader assessing relevance of a retrieved document.\")]\n",
    "        + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    "    )\n",
    "\n",
    "    # Debug: Print the raw response from the model\n",
    "    print(\"Raw Grading Response:\", grading_response.content)\n",
    "\n",
    "    try:\n",
    "        grading_result = json.loads(grading_response.content)\n",
    "        # Debug: Print the parsed JSON\n",
    "        print(\"Parsed JSON:\", grading_result)\n",
    "\n",
    "        # Ensure the key exists and has a valid value\n",
    "        if \"binary_score\" in grading_result and grading_result[\"binary_score\"] in [\"Yes\", \"No\"]:\n",
    "            # Additional check: If the document does not contain the question keywords, force \"No\"\n",
    "            question_keywords = [\"viral kohli\", \"IIT KGP campus\"]\n",
    "            if not any(keyword.lower() in docs_txt.lower() for keyword in question_keywords):\n",
    "                print(\"Question keywords not found in document. Forcing 'No'.\")\n",
    "                return False\n",
    "            return grading_result[\"binary_score\"] == \"Yes\"\n",
    "        else:\n",
    "            print(\"Invalid JSON structure or missing 'binary_score' key.\")\n",
    "            return False\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON Decode Error: {e}\")\n",
    "        return False  # If the model fails to return valid JSON, assume irrelevance.  # If the model fails to return valid JSON, assume irrelevance.\n",
    "\n",
    "# Main Logic for Routing\n",
    "question = \"what is the qualification requirement for tescra achnet software role?\"\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "\n",
    "\n",
    "if is_relevant(docs, question):\n",
    "    docs_txt = format_docs(docs)\n",
    "    rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    answer = generation.content\n",
    "else:\n",
    "    print(\"No relevant context found. Searching the web using TAVIly...\")\n",
    "    tavili_results = tavili_search(question)  # Use TAVIly search\n",
    "    answer = tavili_results  # Use TAVIly results as fallback\n",
    "\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to Generate response only using the context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 58\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# def process_answer(answer):\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#     # Check if the answer contains the specific phrase \"I don't have this knowledge in my database.\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#     if \"I don't have this knowledge in my database\" in answer:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#question = \"Is there a cricket player virat kohli at IIT KGP campus?\"\u001b[39;00m\n\u001b[0;32m     57\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the criteria for getting a branch change?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 58\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(question)\n\u001b[0;32m     59\u001b[0m docs_txt \u001b[38;5;241m=\u001b[39m format_docs(docs)\n\u001b[0;32m     60\u001b[0m rag_prompt_formatted \u001b[38;5;241m=\u001b[39m rag_prompt\u001b[38;5;241m.\u001b[39mformat(context\u001b[38;5;241m=\u001b[39mdocs_txt, question\u001b[38;5;241m=\u001b[39mquestion)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "\n",
    "Here is the context to use to answer the question:\n",
    "\n",
    "{context} \n",
    "\n",
    "Think carefully about the above context. \n",
    "\n",
    "Now, review the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Provide an answer to this questions using only the above context. \n",
    "\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "question = \"what is the criteria for getting a branch change?\"\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "print(generation.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes',\n",
       " 'key': 'Ensure the STUDENT ANSWER is grounded in the FACTS.',\n",
       " 'explanation': \"The context clearly states that the job requires a Bachelor's degree with a Master's preferred. The student answer aligns with this requirement, ensuring it meets all criteria.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "# Hallucination grader instructions\n",
    "hallucination_grader_instructions = \"\"\"\n",
    "\n",
    "You are a teacher grading a quiz. \n",
    "\n",
    "You will be given FACTS and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
    "\n",
    "(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "hallucination_grader_prompt = \"\"\"FACTS: \\n\\n {documents} \\n\\n STUDENT ANSWER: {generation}. \n",
    "\n",
    "Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS. And a key, explanation, that contains an explanation of the score.\"\"\"\n",
    "\n",
    "# Test using documents and generation from above\n",
    "hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(\n",
    "    documents=docs_txt, generation=generation.content\n",
    ")\n",
    "result = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=hallucination_grader_instructions)]\n",
    "    + [HumanMessage(content=hallucination_grader_prompt_formatted)]\n",
    ")\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary_score': 'yes', 'explanation': '...'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Answer Grader\n",
    "\n",
    "# # Answer grader instructions\n",
    "# answer_grader_instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "# You will be given a QUESTION and a STUDENT ANSWER. \n",
    "\n",
    "# Here is the grade criteria to follow:\n",
    "\n",
    "# (1) The STUDENT ANSWER helps to answer the QUESTION\n",
    "\n",
    "# Score:\n",
    "\n",
    "# A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "# The student can receive a score of yes if the answer contains extra information that is not explicitly asked for in the question.\n",
    "\n",
    "# A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "# Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "# Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# # Grader prompt\n",
    "# answer_grader_prompt = \"\"\"QUESTION: \\n\\n {question} \\n\\n STUDENT ANSWER: {generation}. \n",
    "\n",
    "# Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER meets the criteria. And a key, explanation, that contains an explanation of the score.\"\"\"\n",
    "\n",
    "# # Test\n",
    "# question = \"What are the traits of a good man?\"\n",
    "# answer = \"The Llama 3.2 models released today include two vision models: Llama 3.2 11B Vision Instruct and Llama 3.2 90B Vision Instruct, which are available on Azure AI Model Catalog via managed compute. These models are part of Meta's first foray into multimodal AI and rival closed models like Anthropic's Claude 3 Haiku and OpenAI's GPT-4o mini in visual reasoning. They replace the older text-only Llama 3.1 models.\"\n",
    "\n",
    "# # Test using question and generation from above\n",
    "# answer_grader_prompt_formatted = answer_grader_prompt.format(\n",
    "#     question=question, generation=answer\n",
    "# )\n",
    "# result = llm_json_mode.invoke(\n",
    "#     [SystemMessage(content=answer_grader_instructions)]\n",
    "#     + [HumanMessage(content=answer_grader_prompt_formatted)]\n",
    "# )\n",
    "# json.loads(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fucntion to extract and save the embedding in a vectorbase.json file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store saved as JSON!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Extract vectors and document metadata\n",
    "vector_data = {\n",
    "    \"documents\": [doc.page_content for doc in doc_splits],\n",
    "    \"metadata\": [doc.metadata for doc in doc_splits],\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"vectorstore.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vector_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Vector store saved as JSON!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing .json and .txt files, created individual functions for each files (flat and nested .json files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"From the **third semester onwards**, undergraduate students, including 2-year MSc students, have the opportunity to opt for additional subjects, contingent on the established rules and regulations [1]. Additional subjects are designed to allow students to explore areas of interest and acquire knowledge that may be beneficial for their future careers [1]. To be eligible for additional subjects, a student must maintain a **Cumulative Grade Point Average (CGPA) of 7.50 or greater, without any backlogs** [1]. Students are allowed to register for additional subjects even with **EAA (Essential Academic Activities) as a backlog** [2]. The number of additional credits a student can earn is capped at **33% of the credit requirements for their major** [2]. A student's registration for an additional subject is subject to several conditions: they must satisfy any **pre-requisites** for the course, there should be **no timetable conflicts**, and the **class size** must permit their enrolment [2]. Furthermore, a student is permitted to register for a maximum of **5 additional subjects in a single semester** [2]. The Grade Point Average (GPA) for additional subjects is calculated separately and is noted on the grade card, however, **these additional credits do not contribute to the computation of the student's overall CGPA** [2, 3]. Once a student has registered for an additional subject, the grade achieved, including any **F-grade**, will be recorded on their grade card [3].\", 'metadata': {'question': 'What is an additional subject, and what are the rules regarding an additional subject?'}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def process_eatery_data(data):\n",
    "    #food_reviews.json\n",
    "    \"\"\"Processes eatery data and returns a list of dictionaries with review text and metadata.\"\"\"\n",
    "    processed_eatery_reviews = []\n",
    "    for eatery in data:\n",
    "        eatery_name = eatery[\"eatery_name\"]\n",
    "        rating = eatery[\"rating\"]\n",
    "        for review in eatery[\"reviews\"]:\n",
    "            review_text = review[\"text\"]\n",
    "            sentiment = review.get(\"sentiment\")  # Use get to handle missing sentiment\n",
    "            aspects = review.get(\"aspects\", [])  # Use get to handle missing aspects, default to empty list.\n",
    "\n",
    "            processed_eatery_reviews.append({\n",
    "                \"text\": review_text,\n",
    "                \"metadata\": {\n",
    "                    \"eatery\": eatery_name,\n",
    "                    \"rating\": rating,\n",
    "                    \"sentiment\": sentiment,\n",
    "                    \"aspects\": aspects,\n",
    "                },\n",
    "            })\n",
    "    return processed_eatery_reviews\n",
    "\n",
    "def process_faq_data(data):\n",
    "    #tsg_faq.json\n",
    "    \"\"\"Processes FAQ data and returns a list of dictionaries with answer text and metadata.\"\"\"\n",
    "    processed_faqs = []\n",
    "    for faq in data[\"faq\"]:\n",
    "        question = faq[\"question\"]\n",
    "        answer = faq[\"answer\"]\n",
    "        processed_faqs.append({\n",
    "            \"text\": answer,\n",
    "            \"metadata\": {\n",
    "                \"question\": question\n",
    "            }\n",
    "        })\n",
    "    return processed_faqs\n",
    "\n",
    "def process_subject_registration_data(data):\n",
    "    #faq_ug_reg.json\n",
    "    \"\"\"Processes Subject Registration FAQs data and returns a list of dictionaries with reply text and metadata.\"\"\"\n",
    "    processed_subject_registration_qas = []\n",
    "    for category, qa_pairs in data[\"Subject Registration FAQs\"].items():\n",
    "        for qa_pair in qa_pairs:\n",
    "            processed_subject_registration_qas.append({\n",
    "                \"text\": qa_pair[\"reply\"],\n",
    "                \"metadata\": {\n",
    "                    \"question\": qa_pair[\"query\"],\n",
    "                    \"category\": category  # Add category to metadata\n",
    "                }\n",
    "            })\n",
    "    return processed_subject_registration_qas\n",
    "\n",
    "def process_IITKGP_Faqs_data(data):\n",
    "    #iitkgp_faq.json\n",
    "    \"\"\"Processes IIT KGP FAQs data and returns a list of dictionaries with reply text and metadata.\"\"\"\n",
    "    processed_subject_registration_qas = []\n",
    "    for category, qa_pairs in data[\"academic_options\"].items():\n",
    "        for qa_pair in qa_pairs:\n",
    "            processed_subject_registration_qas.append({\n",
    "                \"text\": qa_pair[\"answer\"],\n",
    "                \"metadata\": {\n",
    "                    \"question\": qa_pair[\"question\"],\n",
    "                    \"category\": category  # Add category to metadata\n",
    "                }\n",
    "            })\n",
    "    return processed_subject_registration_qas\n",
    "\n",
    "def process_small_text_files(folder_path):\n",
    "    \"\"\"\n",
    "    Processes all .txt files in a given folder, assuming each contains small texts \n",
    "    (3-4 lines max). Returns a list of dictionaries with text and metadata.\n",
    "    \"\"\"\n",
    "    processed_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    # Remove leading/trailing whitespace and filter out empty lines\n",
    "                    text = \"\\n\".join(line.strip() for line in lines if line.strip()) \n",
    "                    if text:  # Only append if the text is not empty\n",
    "                        processed_texts.append({\n",
    "                            \"text\": text,\n",
    "                            \"metadata\": {\"source\": filename}\n",
    "                        })\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "    return processed_texts\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "# Load and process eatery data\n",
    "if os.path.exists(\"food_review\\food_reviews.json\"):\n",
    "    with open(\"food_reviews.json\", \"r\") as f:\n",
    "        eatery_data = json.load(f)\n",
    "    processed_data.extend(process_eatery_data(eatery_data))\n",
    "\n",
    "# Load and process FAQ data\n",
    "if os.path.exists(\"tsg_faqs.json\"):\n",
    "    with open(\"tsg_faqs.json\", \"r\") as f:\n",
    "        faq_data = json.load(f)\n",
    "    processed_data.extend(process_faq_data(faq_data))\n",
    "\n",
    "# Load and process Subject Registration FAQs data\n",
    "if os.path.exists(\"resources\\faq_ug_reg.json\"):\n",
    "    with open(\"faq_ug_reg.json\", \"r\") as f:\n",
    "        subject_registration_data = json.load(f)\n",
    "    processed_data.extend(process_subject_registration_data(subject_registration_data))\n",
    "\n",
    "if os.path.exists(\"resources\\iitkgp_faqs.json\"):\n",
    "    with open(\"resources\\iitkgp_faqs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        iitkgp_fqs_data = json.load(f)\n",
    "    processed_data.extend(process_IITKGP_Faqs_data(iitkgp_fqs_data))\n",
    "\n",
    "# Load and process text files (example: factual_text.txt)\n",
    "if os.path.exists(\"text_files\"):\n",
    "    processed_data.extend(process_small_text_files(\"text_files\"))\n",
    "\n",
    "# Now processed_data contains all data in a unified format\n",
    "# ... chunking, embedding, and indexing ...\n",
    "\n",
    "print(processed_data[0]) #prints the first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"From the **third semester onwards**, undergraduate students, including 2-year MSc students, have the opportunity to opt for additional subjects, contingent on the established rules and regulations [1]. Additional subjects are designed to allow students to explore areas of interest and acquire knowledge that may be beneficial for their future careers [1]. To be eligible for additional subjects, a student must maintain a **Cumulative Grade Point Average (CGPA) of 7.50 or greater, without any backlogs** [1]. Students are allowed to register for additional subjects even with **EAA (Essential Academic Activities) as a backlog** [2]. The number of additional credits a student can earn is capped at **33% of the credit requirements for their major** [2]. A student's registration for an additional subject is subject to several conditions: they must satisfy any **pre-requisites** for the course, there should be **no timetable conflicts**, and the **class size** must permit their enrolment [2]. Furthermore, a student is permitted to register for a maximum of **5 additional subjects in a single semester** [2]. The Grade Point Average (GPA) for additional subjects is calculated separately and is noted on the grade card, however, **these additional credits do not contribute to the computation of the student's overall CGPA** [2, 3]. Once a student has registered for an additional subject, the grade achieved, including any **F-grade**, will be recorded on their grade card [3].\",\n",
       "  'metadata': {'question': 'What is an additional subject, and what are the rules regarding an additional subject?'}},\n",
       " {'text': 'Students can apply for additional subjects during the **active semester registration period** [3]. The application process involves browsing a list of available subjects and selecting the relevant department from which they wish to take the additional subject [3]. Once a department is selected, a **list of all subjects** offered by that department will become visible [4]. The student then indicates their desired subject [4]. As part of the application, students are required to **submit a brief description explaining their reason for choosing the specific subject** [4]. The **professor-in-charge** of the selected course will review the submitted descriptions and, based on factors such as availability, will either **approve or decline the request** [4]. Students will receive a notification via email regarding the **status of their additional subject application** [4]. Following the approval or denial of applications, students will be able to view a list of all **approved subjects** [5].',\n",
       "  'metadata': {'question': 'What is the process to apply for additional subjects?'}},\n",
       " {'text': 'There are **no supplementary examinations** available for additional subjects [5]. If a student receives an **F grade** in an additional subject, they are required to **repeat the subject** in the next semester it is offered [5]. However, if the subject is not available in the following semester, the student has the option to **choose an alternative additional subject** [5]. It is important to note that additional subjects completed after receiving an F grade or in exchange for an F grade **will not be counted towards a minor specialisation** [6].',\n",
       "  'metadata': {'question': 'What happens if the student receives an F Grade / Fails in an additional subject?'}},\n",
       " {'text': 'Students can drop an additional subject by following a specific procedure, which must be completed **before the mid-semester examination** [6]. Failure to drop the subject before this deadline will result in the subject remaining registered, and it will be reflected on the grade card even if an F grade is received [6]. The process differs based on whether the semester is in offline or online mode. \\n\\nFor **offline semesters**, the process is as follows:  \\n\\n*   Students must submit a written letter to the **Head of Department**, with approval from their faculty advisor, stating their intention to drop the additional subject [7].\\n*   The **approval letter**, signed by the Head of Department and Faculty Advisor, must be submitted to the Academic Section [7].\\n*   A specific date before the mid-semester examination is set for dropping additional subjects, and students must adhere to this deadline [7].\\n*   Students can consult the academic section or Departmental Representative to find the **last date** for dropping an additional subject [7]. Generally, the last date for dropping an additional subject is one week before the mid-semester examination [8].\\n\\nFor **online semesters**, the process is as follows:\\n\\n*   Students must email their **Faculty Advisor** to request to drop the additional subject and obtain their approval [8].\\n*   The same email thread must be forwarded to the **Head of the Department** for their approval [8].\\n*   Once the student has obtained approval from both their faculty advisor and Head of the Department, the complete email thread should be sent to the **Assistant Registrar Academics (arug@adm.iitkgp.ac.in)**, who will then complete the necessary actions [8].\\n\\nFor both offline and online semesters, a **fixed last date** for dropping additional subjects is established at the beginning of each semester. Students can get in touch with the academic section or Departmental Representative to the Institute Senate for the specific deadline to drop an additional subject in the current semester [9].',\n",
       "  'metadata': {'question': 'What is the process to drop additional subjects?'}},\n",
       " {'text': 'Taking additional subjects allows students to explore academic areas that are not part of their regular curriculum and to **develop a broader range of skills** [9]. Additional subjects can also be beneficial in the following ways:\\n\\n*  Students can include these subjects on their **CV** to highlight their interest and knowledge in particular fields [10].\\n*  Additional subjects can assist students who want to **pursue higher education** in a domain that is different from their parent department [10].',\n",
       "  'metadata': {'question': 'What are the benefits of taking an additional subject?'}},\n",
       " {'text': 'To find the slots for any subject, you must first log in to the **ERP system** [10]. Then, navigate to **Academic > Time Table > Subject List with Time Table Slots** [11]. Here, you need to select the semester and the parent department/centre/school offering the subject [11]. A list of all subjects offered by the selected department in the chosen semester will be shown, along with their respective time slots [11].',\n",
       "  'metadata': {'question': 'How can I know the slots for any subject offered by any dept/centre/school?'}},\n",
       " {'text': \"Breadth subjects are an integral part of the curriculum, with each department having specific requirements [11]. These subjects are **compulsory** and are typically **unrelated to the studentâ€™s parent department** [11]. The GPA earned in breadth subjects **contributes to the student's overall GPA** [11]. Breadth electives include areas such as HSS (Humanities and Social Sciences), Management, and IT/Sc/Ent/IPL (Information Technology, Science, Entrepreneurship, Intellectual Property Law) [12].\",\n",
       "  'metadata': {'question': 'What is a Breadth subject?'}},\n",
       " {'text': 'To check for timetable clashes, first, you need to access the **central timetable** in the ERP, which can be found by navigating to **Academic > Time Table > Central Timetable** [12]. This timetable displays the names of all time slots [12]. Next, to check the specific slot of a course, go to **Academic > Time Table > Subject List with Timetable Slots** [12]. Select the department offering the course and scroll through the list to find the specific course and its time slot. For more detailed instructions on understanding the timetable, you can refer to the article at this URL: https://web.scholarsavenue.org/understanding-the-timetable-9971bbab4cb7 [12].',\n",
       "  'metadata': {'question': 'How to check if a course clashes with our timetable?'}},\n",
       " {'text': \"The ERP system **does not permit** registration for courses that have a timetable conflict with a student's schedule [12]. However, in **exceptional circumstances**, students may try to contact their faculty advisor and the concerned faculty member responsible for that course to explore potential options [13].\",\n",
       "  'metadata': {'question': 'Is there any way to apply for courses that clash with our timetable but weâ€™re highly interested in it?'}},\n",
       " {'text': 'There is **no direct method** to take an approved additional course as a breadth subject [13]. However, if the additional course serves as a **pre-requisite for a micro-specialization**, it might be possible to use it as a breadth subject, provided that it also features on the list of available breadth subjects [13]. Additionally, students can consult their faculty advisor to explore this possibility; with their approval, additional courses can be considered as breadth subjects [14].',\n",
       "  'metadata': {'question': 'What is the procedure to take an approved additional course as breadth instead?'}},\n",
       " {'text': 'Typically, **changing an allotted breadth subject is not permitted**, except in specific circumstances such as a change of branch, a backlog, or a timetable conflict with an additional subject [14]. To request a change in breadth subject, the student must email the course faculty and their faculty advisor with a suitable justification, and the change is only possible if they grant their approval [14].',\n",
       "  'metadata': {'question': 'Can I change a breadth subject allotted to me?'}},\n",
       " {'text': \"A **minimum CGPA of 7.5, without any backlogs**, is required to be eligible for taking an additional subject [15]. For breadth subjects, students are shortlisted for specific courses based on their CGPA [15]. While additional course availability is generally independent of a student's department, the selection of breadth courses is dependent on the studentâ€™s department [15]. When an additional course has high demand, the faculty in charge may use a rule (typically CGPA) to select students [15]. If a student is extremely interested in a specific course, it is recommended that they get in contact with the concerned faculty [15].\",\n",
       "  'metadata': {'question': 'Do my department and CGPA have any impact on the additional or breadth courses Iâ€™m able to procure?'}},\n",
       " {'text': 'To find the list of courses needed for a Minor, log into the **ERP system**, and go to **Academic > Subjects > Minor Curricula** [16]. For the list of courses required for a Micro Specialization, log in to the ERP system and go to **Academic > UG > Student Academic Activities (UG) > Click to view document for Micro Specialization** [16]. It is important to note that courses completed before applying for a Minor are valid towards the minor, but courses completed before applying for a Micro Specialization do not count toward it [16].',\n",
       "  'metadata': {'question': 'Where can we find the list of courses necessary for a particular minor/micro? If we have completed those courses before applying for minor/micro, do they still count?'}},\n",
       " {'text': 'Applying for additional subjects is **not mandatory** for internships or placements but is often recommended [17]. It is advisable for students to contact professors and senior students to identify courses that may be beneficial in their chosen field of interest [17].',\n",
       "  'metadata': {'question': 'Is it necessary to apply for additional subjects in terms of internship/placement?'}},\n",
       " {'text': 'Generally, applying for courses not listed in the department-specific breadth course list is **not permitted** [17]. However, if there is a compelling reason or strong incentive to do so, students can consult their **Faculty Advisor** and the concerned faculty for the course [17].',\n",
       "  'metadata': {'question': 'Is it possible to apply for courses absent in the department-specific breadth course list but still available in KGP? What is the procedure to do that?'}},\n",
       " {'text': 'A Double Major allows all eligible, undergraduate students to earn a second major in a branch that is different from the B.Tech(Hons.)/B.S.(Hons.) branch in which he/she takes admission. For example, a student who takes admission in B.Tech (Hons.) in Electrical Engineering can earn a Double Major, say, in Computer Science and Engineering, or a student who takes admission in B.S(Hons.) in Physics can earn a Double Major, say, in Electronics and Electrical Communications Engineering.',\n",
       "  'metadata': {'question': 'What is a Double Major?',\n",
       "   'category': 'double_major'}},\n",
       " {'text': 'Any academic unit including departments, centres and schools can offer the Double Major programs.',\n",
       "  'metadata': {'question': 'Who can offer a Double Major?',\n",
       "   'category': 'double_major'}},\n",
       " {'text': 'In each academic year, the academic units can specify the maximum number of seats for Double Major program along with any other appropriate selection criteria. The minimum number of seats offered will be 10 % of the sanctioned UG seat strength or 10 seats, whichever is minimum.',\n",
       "  'metadata': {'question': 'What is the number of seats in each Double Major program?',\n",
       "   'category': 'double_major'}},\n",
       " {'text': 'The Double Major curriculum consists of:\\n- Core subjects (theory and sessional/laboratory)\\n- Depth Electives from the respective academic unit or other units\\n- A project work of 3 credits or a substitute Depth Elective course of 3-4 credits',\n",
       "  'metadata': {'question': 'What are the curricular structure for a Double Major?',\n",
       "   'category': 'double_major'}},\n",
       " {'text': 'The total number of credits required for a secondary Double Major is 40-48, which includes:\\n- At least 27 credits for theory\\n- 3 credits for the project or substitute Depth Elective course\\n- 8 credits for sessional subjects',\n",
       "  'metadata': {'question': 'How many total credits are required for a Double Major?',\n",
       "   'category': 'double_major'}},\n",
       " {'text': 'No, the 40-48 credits for the Double Major must be separate and should not overlap with the 160-168 credits required for the B.Tech(Hons.)/ BS(Hons.) degree.',\n",
       "  'metadata': {'question': 'Can Double Major credits overlap with the primary degree course (B.Tech/B.S.) credits?',\n",
       "   'category': 'double_major'}},\n",
       " {'text': 'Students can opt for a Double Major at the end of their 2nd semester. If there are vacant seats, applications may also be accepted at the end of the 3rd semester.',\n",
       "  'metadata': {'question': 'When can I apply for a Double Major?',\n",
       "   'category': 'double_major'}},\n",
       " {'text': '- A minimum CGPA of 7.0 without any backlog at the time of application.\\n- Continuously maintaining a backlog-free status throughout the program.\\n- Academic Units may also specify pre-requisites for Double Major.',\n",
       "  'metadata': {'question': 'What are the eligibility criteria for a Double Major?',\n",
       "   'category': 'double_major'}},\n",
       " {'text': 'Yes, students can withdraw from the Double Major program (after enrolling from Double Major) at any point of time.',\n",
       "  'metadata': {'question': 'Can I withdraw from the Double Major program?',\n",
       "   'category': 'double_major'}},\n",
       " {'text': 'Academic units may specify pre-requisite subjects. If these are from the 1st-year curriculum, the number of such pre-requisites will be limited to 1 or 2.',\n",
       "  'metadata': {'question': 'Are there any pre-requisites for the Double Major?',\n",
       "   'category': 'double_major'}},\n",
       " {'text': 'The transcript and the degree certificate will clearly state that any extra semester(s) taken to complete the course are not due to any backlog. It will be phrased as “B.Tech(Hons.)/BS(Hons.) in [PARENT ACADEMIC STREAM] and Major in [SECONDARY ACADEMIC STREAM] (with Specialization, if applicable in [SPECIALIZATION]).”',\n",
       "  'metadata': {'question': 'How will the Double Major be reflected in my transcript?',\n",
       "   'category': 'double_major'}},\n",
       " {'text': 'If you complete the requirements for the primary degree but not the Double Major, you will graduate with only the B.Tech(Hons.) / BS(Hons.) degree. However, if you have completed the requirements for a Minor in the process, you may be awarded a Minor instead of the Double Major.Otherwise the extra subjects registered and cleared under double major will be shown as Additional subjects with Additional CGPA.',\n",
       "  'metadata': {'question': 'What happens if I complete my primary degree requirements but not the Double Major requirements?',\n",
       "   'category': 'double_major'}},\n",
       " {'text': 'The subjects for double major will be normally scheduled along with other regular subjects. The institute may consider reserving special timetable slots and offering summer courses to accommodate Double Major students.',\n",
       "  'metadata': {'question': 'How will subjects for the Double Major be scheduled?',\n",
       "   'category': 'double_major'}},\n",
       " {'text': 'From the admission year 2024, there is no possibility of changing branch from one program to another. However, undergraduate students admitted through JEE Advanced from the Admission Year 2024 can earn a \"Double Major.\"',\n",
       "  'metadata': {'question': 'Can I change my branch or program?',\n",
       "   'category': 'switchover_dual_degree'}},\n",
       " {'text': 'Students admitted to the 4-year B.Tech.(Hons.) and BS (Hons.) through JEE (Advanced) are eligible for Switch Over to a Dual Degree Program in their parent department or in an interdisciplinary program such as:\\n- Entrepreneurship\\n- Financial Engineering\\n- Artificial Intelligence & Machine Learning\\n- Petroleum Engineering\\n- Within their parent department',\n",
       "  'metadata': {'question': 'What are the options for switching over to Dual Degree programs?',\n",
       "   'category': 'switchover_dual_degree'}},\n",
       " {'text': 'Eligible students must apply:\\n- For interdisciplinary programs, during the Spring Semester of their 2nd year or 3rd year, depending on the interdisciplinary Dual Degree program (see below for details).\\n- For Dual Degree program within their parent department, during the Spring semester of their 3rd year.\\n\\nFor both cases, applications should be submitted online through ERP by the specified deadline.',\n",
       "  'metadata': {'question': 'How and when can I apply for a Switch Over?',\n",
       "   'category': 'switchover_dual_degree'}},\n",
       " {'text': '- B.Tech.(Hons.) in any Engineering/ B.S. (Hons.) in any Science Discipline and M.Tech. in Financial Engineering (5-year)- Inter-disciplinary\\n- B.Tech.(Hons.) in any Engineering/ B.S. (Hons.) in any Science Discipline and M.Tech. in Engineering Entrepreneurship (5-year)- Inter-disciplinary\\n- B.Tech.(Hons.) in any Engineering/ B.S. (Hons.) in any Science Discipline and M.Tech. in Artificial Intelligence & Machine Learning (5-year)- Inter-disciplinary\\n- B.Tech.(Hons.) in Mechanical/Chemical/Mining Engineering OR B.S.(Hons.) in Geology/Geophysics and M.Tech. in Petroleum Engineering (5-year)-Trans-disciplinary',\n",
       "  'metadata': {'question': 'What are the various interdisciplinary programs?',\n",
       "   'category': 'switchover_dual_degree'}},\n",
       " {'text': 'Dual Degree Programs in Engineering Entrepreneurship, Financial Engineering (5-year):\\n- Students must be at the end of their 2nd year.\\n- Must have completed all prescribed coursework up to the 4th Semester in the first attempt with a minimum GPA of 6.00.\\n- For Financial Engineering, the total student strength is increased to 60.\\n- Allocation is made based on CGPA at the end of 2nd semester\\n\\nDual Degree in Artificial Intelligence and Machine Learning (AIML) (5-year):\\n- Students must be at the end of their 3rd year.\\n- Must have completed all prescribed coursework up to the 6th Semester in the first attempt with a minimum GPA of 6.00.\\n- Selection is based on CGPA with initial allocation of 4 seats per participating department, followed by filling remaining seats based on CGPA irrespective of department.\\n\\nTransdisciplinary Petroleum Engineering (5-year):\\n- Students from Mechanical Engineering, Chemical Engineering, Mining Engineering, and 4-year B.S in Geology/Geophysics can switch to M.Tech in Petroleum Engineering with B.Tech in the parent department.\\n- Students can opt for this at the end of the 2nd year if there are vacant seats after the first-year allocation.',\n",
       "  'metadata': {'question': 'What are the eligibility criteria for switching over to interdisciplinary programs?',\n",
       "   'category': 'switchover_dual_degree'}},\n",
       " {'text': 'B.Tech.(Hons.) to M.Tech. Dual Degree & B.S. (Hons.) to MS Dual Degree Switch Over:\\n- Third-year B.Tech.(Hons.)/B.S. (Hons.) students can switch to a 5-year Dual Degree course in the same department.\\n- Must have completed all curricular requirements up to the 6th semester without any backlogs and a minimum CGPA of 6.00.',\n",
       "  'metadata': {'question': 'What are the eligibility criteria for switching over to a Dual Degree in the same department?',\n",
       "   'category': 'switchover_dual_degree'}},\n",
       " {'text': '- Third-year Dual Degree students in Agricultural and Food Engineering, Civil Engineering, Electrical Engineering, Electronics & Electrical Communication Engineering, Mechanical Engineering and Mathematics and Computing can choose their M.Tech. specialization.\\n- Students can choose any approved specialization or, if the department limits seats per specialization, allocation will be based on choice and CGPA at the end of the 3rd year.',\n",
       "  'metadata': {'question': 'How can Dual Degree students choose their specialization?',\n",
       "   'category': 'switchover_dual_degree'}},\n",
       " {'text': 'Students with an appropriate Cumulative Grade Point Average (CGPA) and without any backlog are eligible to take additional subjects from the third semester onwards. This also includes 2-year MSc students.',\n",
       "  'metadata': {'question': 'Who is eligible to take additional subjects from the third semester onwards?',\n",
       "   'category': 'additional_credits'}},\n",
       " {'text': 'Eligible students can take additional subjects to earn up to 33% of the credits required for their major.',\n",
       "  'metadata': {'question': 'How many additional credits can a student take?',\n",
       "   'category': 'additional_credits'}},\n",
       " {'text': 'Yes, students can opt for additional subjects both within and outside their own discipline.',\n",
       "  'metadata': {'question': 'Can additional subjects be from outside my discipline?',\n",
       "   'category': 'additional_credits'}},\n",
       " {'text': 'Students can register for additional subjects only if they meet the prerequisites, there is no clash in the timetable, and the class size is within permissible limits.',\n",
       "  'metadata': {'question': 'What are the conditions for registering for additional subjects?',\n",
       "   'category': 'additional_credits'}},\n",
       " {'text': 'No, the grades obtained in additional subjects will be recorded on the grade card, but the credits from these additional subjects will not contribute to the computation of the CGPA. However, it will be counted for the Double Major, if the student has registered for Double Major and the subjects fall under the Double Major curriculum.',\n",
       "  'metadata': {'question': 'Will the grades of additional subjects affect my CGPA?',\n",
       "   'category': 'additional_credits'}},\n",
       " {'text': 'The GPA for additional subjects is calculated separately and will be indicated on the grade card.',\n",
       "  'metadata': {'question': 'How will the GPA for additional subjects be calculated?',\n",
       "   'category': 'additional_credits'}},\n",
       " {'text': 'Yes, a student can drop an additional subject before the mid-semester examination following the proper procedure. If not dropped properly, the subject will remain registered and will appear on the Grade Card even if an F grade is received.',\n",
       "  'metadata': {'question': 'Can I drop an additional subject after registering?',\n",
       "   'category': 'additional_credits'}},\n",
       " {'text': 'No, there is no provision for a supplementary examination for additional subjects.',\n",
       "  'metadata': {'question': 'Is there a supplementary examination for additional subjects?',\n",
       "   'category': 'additional_credits'}},\n",
       " {'text': 'If a student fails an additional subject, they must retake it in the next semester it is offered. If the subject is not offered in the next semester, the student may choose an alternative additional subject.',\n",
       "  'metadata': {'question': 'What happens if I fail an additional subject?',\n",
       "   'category': 'additional_credits'}},\n",
       " {'text': 'Yes, students can register for additional subjects even if they have an EAA (Extra Academic Activities) backlog.',\n",
       "  'metadata': {'question': 'Can I register for additional subjects if I have an EAA backlog?',\n",
       "   'category': 'additional_credits'}},\n",
       " {'text': 'Micro-Credit courses are one-credit courses lasting 3-14 days, offered during the semester or in available free slots. They can be held during evenings, weekdays, or weekends.',\n",
       "  'metadata': {'question': 'What are Micro-Credit courses?',\n",
       "   'category': 'micro_credits'}},\n",
       " {'text': 'The grade obtained will be mentioned in the transcript/grade card and used to compute a separate CGPA (ACGPA). There are no supplementary exams for these courses. In case of failure, students must repeat the course if offered or take an alternative Micro-Credit course in the next semester.',\n",
       "  'metadata': {'question': 'How are Micro-Credit courses graded and counted?',\n",
       "   'category': 'micro_credits'}},\n",
       " {'text': 'Students are normally not expected to take more than 2 such courses in a semester. These courses may also count towards minor and micro-specialization requirements.',\n",
       "  'metadata': {'question': 'How many Micro-Credit courses can I take?',\n",
       "   'category': 'micro_credits'}},\n",
       " {'text': 'A Minor is an additional set of subjects offered by a department in which it already offers a major. It allows students to gain expertise in another area alongside their major.',\n",
       "  'metadata': {'question': 'What is a Minor in a discipline?',\n",
       "   'category': 'minor_in_discipline'}},\n",
       " {'text': 'A Minor requires students to take six or more subjects, amounting to 18-24 contact hours plus 3-9 hours of laboratory work. These subjects will be a mix of mostly core subjects and some electives.',\n",
       "  'metadata': {'question': 'How many subjects are required for a Minor?',\n",
       "   'category': 'minor_in_discipline'}},\n",
       " {'text': 'Students must register for a Minor at the beginning of their 5th semester.',\n",
       "  'metadata': {'question': 'When can students register for a Minor?',\n",
       "   'category': 'minor_in_discipline'}},\n",
       " {'text': 'Students must have a CGPA of 7.0 or above without any backlog to register for a Minor. Additionally, they must maintain in subsequent semesters no backlog to keep the Minor registration active.',\n",
       "  'metadata': {'question': 'What are the eligibility criteria to register for a Minor?',\n",
       "   'category': 'minor_in_discipline'}},\n",
       " {'text': 'Students must score a minimum grade point average of 6.00 in all subjects that constitute the requirement for a Minor to be awarded the Minor in that discipline. The CGPA, however, should be maintained at a minimum of 8.0.',\n",
       "  'metadata': {'question': 'What is the minimum grade point average required to be awarded a Minor?',\n",
       "   'category': 'minor_in_discipline'}},\n",
       " {'text': 'Yes, students enrolled in Dual Degree (B.Tech.-M.Tech.), B.Tech. (Hons.), B.S. (Hons.), B.S.-M.S. and B.Arch. (Hons.) programs can pursue a Minor in another discipline up to their 10th semester. B.Tech. (Hons.) & B.S. students should complete the Minor requirements by their 8th semester.',\n",
       "  'metadata': {'question': 'Are Dual Degree, Integrated M.Sc., and B.Arch. (Hons.) students allowed to pursue a Minor?',\n",
       "   'category': 'minor_in_discipline'}},\n",
       " {'text': 'Yes, students can register for a Minor even if they have an EAA (Extra Academic Activities) backlog.',\n",
       "  'metadata': {'question': 'Can students register for a Minor if they have an EAA backlog?',\n",
       "   'category': 'minor_in_discipline'}},\n",
       " {'text': 'A Micro-Specialization is a focused area of study within an undergraduate program, consisting of a sequence of courses and projects that provide specialized knowledge and skills.',\n",
       "  'metadata': {'question': 'What is a Micro-Specialization?',\n",
       "   'category': 'micro_specialization'}},\n",
       " {'text': 'Each Micro-Specialization is structured into three sequential components:\\n- *Component-I:* One Foundation Course (2-4 credits) that is mandatory and serves as a prerequisite for the subsequent components.\\n- *Component-II:* One or two subjects (3-4 credits each) from a specified list.\\n- *Component-III:* A Project/Design/Term Paper (4 credits) or one subject (4 credits) from a specified list.',\n",
       "  'metadata': {'question': 'What is the structure of a Micro-Specialization?',\n",
       "   'category': 'micro_specialization'}},\n",
       " {'text': 'Students must complete 3-4 subjects, amounting to 10-14 credits, from the specified list to earn a Micro-Specialization.',\n",
       "  'metadata': {'question': 'How many subjects and credits are required to complete a Micro-Specialization?',\n",
       "   'category': 'micro_specialization'}},\n",
       " {'text': 'The required subjects for a Micro-Specialization can be taken as Breadth/Open Electives or Additional Subjects. If specified in the Micro-Specialization curriculum, they can also be taken as Micro-Credits.',\n",
       "  'metadata': {'question': 'How can the subjects for a Micro-Specialization be taken?',\n",
       "   'category': 'micro_specialization'}},\n",
       " {'text': 'Students can register for a Micro-Specialization at the beginning of any semester beyond the first year.',\n",
       "  'metadata': {'question': 'When can students register for a Micro-Specialization?',\n",
       "   'category': 'micro_specialization'}},\n",
       " {'text': 'To register for a Micro-Specialization, students must have completed all curricular requirements up to the previous semester and have a CGPA of at least 7.0. After registering, they must not have any backlog to keep the registration active.',\n",
       "  'metadata': {'question': 'What are the eligibility criteria for registering for a Micro-Specialization?',\n",
       "   'category': 'micro_specialization'}},\n",
       " {'text': 'A minimum GPA of 6.00 is required for the subjects contributing to the Micro-Specialization to earn the Micro-Specialization.',\n",
       "  'metadata': {'question': 'What is the minimum GPA required to earn a Micro-Specialization?',\n",
       "   'category': 'micro_specialization'}},\n",
       " {'text': 'Yes, students can register for Micro-Specialization subjects even if they have an EAA (Extra Academic Activities) backlog (Senate Item 322.H.I.C.6).',\n",
       "  'metadata': {'question': 'Can students register for Micro-Specialization subjects if they have an EAA backlog?',\n",
       "   'category': 'micro_specialization'}},\n",
       " {'text': 'The SAP allows students of 4-year B.Tech. (Hons.) and B.S. (Hons.) programs to pursue a full-semester internship in the 7th semester, either in India or abroad, in corporate/industrial sectors, research organizations, or academic institutes.',\n",
       "  'metadata': {'question': 'What is the Semester Away Program (SAP)?',\n",
       "   'category': 'semester_away_program_sap'}},\n",
       " {'text': 'Students avail the SAP in their 7th semester. However, they have to apply for the same in ERP in the 6th semester with all details and documents.',\n",
       "  'metadata': {'question': 'When can students avail the SAP?',\n",
       "   'category': 'semester_away_program_sap'}},\n",
       " {'text': 'The SAP is equivalent to the corresponding credits of 7th semester.',\n",
       "  'metadata': {'question': 'How many credits is the SAP equivalent to?',\n",
       "   'category': 'semester_away_program_sap'}},\n",
       " {'text': 'Students who avail of the SAP can register directly for BTP-II without doing BTP-I. However, it will be termed as BTP.',\n",
       "  'metadata': {'question': 'What happens to BTP-1 if a student opts for the SAP?',\n",
       "   'category': 'semester_away_program_sap'}},\n",
       " {'text': 'Yes, the SAP can be pursued either in India or abroad.',\n",
       "  'metadata': {'question': 'Can the SAP be carried out internationally?',\n",
       "   'category': 'semester_away_program_sap'}},\n",
       " {'text': 'Students can intern with corporate/industrial sectors, research organizations, or academic institutes.',\n",
       "  'metadata': {'question': 'What kinds of organizations can students intern with during the SAP?',\n",
       "   'category': 'semester_away_program_sap'}},\n",
       " {'text': 'Students who switch to a dual degree program after opting for SAP will be withdrawn from SAP. Students need to register for all 7th-semester courses of the dual degree program but can apply for the Semester Away Project Program (SAPP) if they meet the eligibility norms.',\n",
       "  'metadata': {'question': 'What if a student decides to switch to a dual degree program after opting for SAP?',\n",
       "   'category': 'semester_away_program_sap'}},\n",
       " {'text': 'A: Students who withdraw from SAIP will be treated under the program without SAP. Students must register for all 7th-semester courses of the UG program without SAP.',\n",
       "  'metadata': {'question': 'What happens if a student withdraws from SAP?',\n",
       "   'category': 'semester_away_program_sap'}},\n",
       " {'text': 'Students with unsatisfactory performance in SAP will be treated like those without SAP. In this case, the students must complete all 7th-semester courses of the regular program without SAP, which may require extra semesters depending on the courses needed for graduation.',\n",
       "  'metadata': {'question': 'What if a student’s performance in SAP is unsatisfactory?',\n",
       "   'category': 'semester_away_program_sap'}},\n",
       " {'text': 'A: The performance in SAP will be evaluated by the host supervisor, who will directly send the grades to the department and will be indicated in the 7th-semester transcript. However, at the end of the SAP, the student has to submit a copy of the internship report certified by the host supervisor in the department.',\n",
       "  'metadata': {'question': 'How is the final evaluation for SAP conducted?',\n",
       "   'category': 'semester_away_program_sap'}},\n",
       " {'text': 'A student selected for SAP will pay the appropriate semester registration fees as decided by the competent authority.',\n",
       "  'metadata': {'question': 'Are there any additional fees for participating in SAP?',\n",
       "   'category': 'semester_away_program_sap'}},\n",
       " {'text': 'Disciplinary issues arising while the student is away from the campus will be handled as per the institute’s and host institute’s rules and regulations.',\n",
       "  'metadata': {'question': 'How are disciplinary issues handled during SAP?',\n",
       "   'category': 'semester_away_program_sap'}},\n",
       " {'text': 'Yes, SAP can be a continuation of the mandatory 8-week summer internship that students undertake after completing the 6th semester. The grades for the Summer Internship and the SAP must be submitted separately by the concerned host supervisor.',\n",
       "  'metadata': {'question': 'Can SAP be a continuation of the mandatory 8-week summer internship?',\n",
       "   'category': 'semester_away_program_sap'}},\n",
       " {'text': 'The SAPP allows students of 5-year Dual Degree B.Tech.-M.Tech./B.S.-M.S. programs to pursue a full-semester project in their 9th semester, either in India or abroad, in corporate/industrial sectors, research organizations, or academic institutes.',\n",
       "  'metadata': {'question': 'What is the Semester Away Project Program (SAPP)?',\n",
       "   'category': 'semester_away_project_program_sapp'}},\n",
       " {'text': 'Students avail the SAPP in their 9th semester. However, they have to apply for the same in ERP in the 8th semester with all details and documents.',\n",
       "  'metadata': {'question': 'When can students avail the SAPP?',\n",
       "   'category': 'semester_away_project_program_sapp'}},\n",
       " {'text': 'The SAPP is equivalent to entire 9th semester credit of the respective programme.',\n",
       "  'metadata': {'question': 'How many credits is the SAPP equivalent to?',\n",
       "   'category': 'semester_away_project_program_sapp'}},\n",
       " {'text': 'Yes, the SAPP can be pursued either in India or abroad, subject to approval of the competent authority.',\n",
       "  'metadata': {'question': 'Can the SAPP be carried out internationally?',\n",
       "   'category': 'semester_away_project_program_sapp'}},\n",
       " {'text': 'Students can work with corporate/industrial sectors, research organizations, or academic institutes during the SAPP.',\n",
       "  'metadata': {'question': 'What kinds of organizations can students work with during the SAPP?',\n",
       "   'category': 'semester_away_project_program_sapp'}},\n",
       " {'text': 'Students who initially opt for SAPP but later withdraw will be treated under the regular program without SAPP. They must complete all 9th semester courses of the regular program without SAPP, which may require additional semesters to graduate.',\n",
       "  'metadata': {'question': 'What happens if a student initially opts for SAPP but later decides to withdraw?',\n",
       "   'category': 'semester_away_project_program_sapp'}},\n",
       " {'text': \"If a student's performance in SAPP is unsatisfactory, they will be treated on par with students without SAPP. They must complete all 9th semester courses of the regular program without SAPP, which may require additional semesters to graduate.\",\n",
       "  'metadata': {'question': \"What happens if a student's performance in SAPP is found to be unsatisfactory?\",\n",
       "   'category': 'semester_away_project_program_sapp'}},\n",
       " {'text': \"The final evaluation for SAPP is conducted by the PG academic committee of the concerned Academic unit (IIT Kharagpur) by the 1st week of December through an in-person viva voce. Before that, the student must submit a final report duly certified by the institute and host supervisors. A suitable grade will be awarded for SAPP, considering the report, supervisors' (IIT and host) assessment, and performance in viva voce. The grade will be indicated in the transcript under semester 9.\",\n",
       "  'metadata': {'question': 'How is the final evaluation conducted for SAPP?',\n",
       "   'category': 'semester_away_project_program_sapp'}},\n",
       " {'text': 'Depending on requirements, the department may conduct mid-semester evaluations using an appropriate mode.',\n",
       "  'metadata': {'question': 'Will there be mid-semester evaluations for SAPP?',\n",
       "   'category': 'semester_away_project_program_sapp'}},\n",
       " {'text': 'Yes, students selected for SAPP will pay the appropriate semester registration fees as decided by the competent authority.',\n",
       "  'metadata': {'question': 'Are there additional fees for participating in SAPP?',\n",
       "   'category': 'semester_away_project_program_sapp'}},\n",
       " {'text': \"Any disciplinary issues that arise while the student is away from campus will be handled according to the institute's and host institute's regulations.\",\n",
       "  'metadata': {'question': 'How are disciplinary issues handled during SAPP?',\n",
       "   'category': 'semester_away_project_program_sapp'}},\n",
       " {'text': 'SAPP can potentially be a continuation of the mandatory 8-week summer internship; which students undertake after the completion of the 8th semester. However, there should be no commitment from either the student or the host for clubbing the mandatory internship with SAPP at the beginning, considering the approval process uncertainties. Also, separate evaluations must be carried out for the summer internship and SAPP, with corresponding separate grade submissions.',\n",
       "  'metadata': {'question': 'Can SAPP be a continuation of the mandatory 8-week summer internship?',\n",
       "   'category': 'semester_away_project_program_sapp'}},\n",
       " {'text': 'Academic Units Academic Units Academic Calendar Academic Units Announcements Anti-Ragging Measures Apna IIT KGP Central Library Communication Directory Counselling Centre Degree Verification/ Transcripts/ Certificates Fee Payment (passed-out students only) Rajbhasha Vibhag Right to Information Tenders Vendor Registration under GST Contact Academic Section (approved by Competent Authority as on November 22, 2023) Responsible Officer(s) : Biswajit Bhattacharyya , Joint Registrar, ✉ jracad@adm.iitkgp.ac.in,  ☎ 82075 Santanu Das , Deputy Registrar on Contract, ✉ arug@adm.iitkgp.ac.in, ☎ 82054 Nilamadhab Mishra , Assistant Registrar, ✉ arpgsr@adm.iitkgp.ac.in,  ☎ 81076 Unit Activities Staff Member(s) Contact Details Institute Academic Reception Unit To act as a point of contact for all enquires; Distribution of enquires to the concerned unit/department along with follow up of the same; Dissemination of information to all stakeholders including students, guardians and employees. Arunabha Das ☎ 82182, 82050 ✉ help_ug@adm.iitkgp.ac.in, ✉ help_pg@adm.iitkgp.ac.in Admission & Registration Unit Subject Registration; Fees collection; Admission and Verification of documents; Academic Schedule; Academic Calendar; Time table Tirtha Prasad Saha, Indira Sharma, Purnima Mahadani, Goutom Kumar, Ujjal Das, Aniket Maity ☎ 82060, 82077 ✉ stu_reg@adm.iitkgp.ac.in PG/Ph.D & Academic Research Unit All activities related to PG/PhD/Research Purnima Mahadani, Vibha Chaubey, Sujit Majumder, Sumit Rajan, Arunabha Das ☎ 82082, 82076 ✉ rs_phd@adm.iitkgp.ac.in Examination & Result Unit Conducting & Scheduling of examination at all level; Compilation, validation, publiction of results; Issue of Provisional Certificates, Degree Certificates, etc; Degree Varification; Processing of Electronics Transcript and degree, etc. Tirtha Prasad Saha, Indira Sharma, K. Sudha, Somnath Maiti, Rupa Santra, Shouvik Adhikary, Priti Mondal ☎ 82074, 82080 ✉ exam@adm.iitkgp.ac.in Fellowship & Scholarship Unit Procesing of Fellowships / Scholarships, other expenses of PMRF, etc Indira Sharma, K. Sudha, Subhra Das, Sumit Rajan, Ujjal Das, Priti Mandal, Aniket Maity ☎ 82079 ✉ fellowship@adm.iitkgp.ac.in Academic Meeting & Coordination Unit All Academic Meetings including Senate; Co-ordination with various stake holders; Student disciplinary matters Soumadeep Santra, Smit Rajan, Aniket Maity ☎ 82071 ✉ meeting_acad@adm.iitkgp.ac.in Please dial +91-3222-2 before the phone number if you are not calling through institute landline phone',\n",
       "  'metadata': {'source': 'iitkgp_academic_units.txt'}},\n",
       " {'text': 'Assistantships Assistantships Academic Calendar Academic Units Assistantships Branch Change Certificates and Documents Curricula (UG) Curriculum (PG) FAQ Gymkhana HMC HMC Rules and Regulations Library Facility Minor/Micro PMRF Prize, Medals & Awards Scholarships Student Brotherhood Fund Students Grievance Redressal Committee Switch Over Assistantships Students selected on the basis of All India Joint Entrance Examination for the Dual Degree program or registered in the Dual Degree program in the 2nd year, as a result of change of branch would be considered eligible for the assistantship in the 5th Year (9th and 10th semester) of the Dual Degree program. The student must get CGPA of 8.0 at the end of 8th semester for availing the assistantship. The students must have cleared all the subjects with minimum “P” Grade till the preceding semester (8th semester) and should not have any backlog(s). The assistantship for students with backlog subjects will be withheld and may be subsequently released only after clearance of backlog subject(s) with a minimum CGPA of 8.00 on approval of UGPEC. The admissible amount of assistantship as decided by the MHRD / Institute Authorities will be paid for a maximum period of 12 months, w.e.f., the date of registration in the 9th Semester, as per Academic Calendar. The fees to be paid by the Dual Degree students will be the same as applicable to the regular 2-Year M.Tech. Students irrespective of their receiving any assistantship/scholarship. The tuition fee and fee-waiver will be governed as per terms and conditions enforced by the MHRD from time to time. The Dual Degree students receiving assistantship will not be entitled for any vacation during semesters, summer or winter. A pro-rata deduction of scholarship will be made for leave availed beyond the admissible entitlement, applicable under Postgraduate Regulations. The general rules and regulations applicable for 2-Year M.Tech. Postgraduate students will also be applicable to students pursuing Dual Degree Program. Dual Degree students in the 5th Year (9th and 10th semester) will not be eligible for any other Undergraduate Scholarship(s)/Assistantship, including MCM Scholarship and Financial Assistance. The students will be entitled to avail only one Scholarship/Assistantship, at a time from the Institute or from any other sources. Students in their own interest be encouraged to appear and qualify in the GATE examination conducted by the MHRD, to be considered for the assistantship, in view of the guidelines issued from time-to-time. Scholarship / Assistantship TO 2-Year M.Sc. of Joint M.Sc.- Ph.D. SENATE: 295.H.I.7 The following financial assistance will be available to students enrolled to 2 year M.Sc. Degree of the Joint M.Sc. - Ph.D. Program. Merit- cum-Means Scholarship of Rs. 1,000/- per month up to 25% of students. SC/ST students whose parental income is not more than Rs. 50,920/-per annum (as per the guidelines issued by the Government of India) are entitled to get free seat in the hostel, free basic menu and Rs. 250/- per month as pocket allowance. All admitted students have to pay tuition fee. (BOG: MS/B-(191)/2018/2970) dt 25th Oct 2018. Students not covered under Section – i and Section -ii will be paid scholarship of Rs. 1,000/- per month for the first 4 semesters provided they give an undertaking to continue with the Ph.D. program. A student enrolled for Ph.D. Degree will get a fellowship of Rs. 12,000/- per month for the first two years of his Ph.D. program and Rs. 14,000/- per month for next two years, provided he/she has a CGPA of 8.0 or more at the end of 4th semester, otherwise he/she has to qualify in GATE/NET/NBHM. The fellowship rates may be revised by the Institute from time to time.',\n",
       "  'metadata': {'source': 'iitkgp_assistanships.txt'}},\n",
       " {'text': 'Minor/Micro Minor/Micro Academic Calendar Academic Units Assistantships Branch Change Certificates and Documents Curricula (UG) Curriculum (PG) FAQ Gymkhana HMC HMC Rules and Regulations Library Facility Minor/Micro PMRF Prize, Medals & Awards Scholarships Student Brotherhood Fund Students Grievance Redressal Committee Switch Over Micro-Credits Micro-Credit courses are one credit courses having duration of 3‐14 days offered during the semester or during evening or free slots available during weekdays or even weekends that are not declared holidays in some special cases when students are available. The subject will be counted for total credit requirement for completion of degree in special cases subject to recommendation of the Head and approval of concerned Academic Dean with due reasons recorded. The subject and grade will be mentioned in transcript / grade card and will be used to compute CGPA for additional subjects and will be shown as ACGPA. Failures will be marked by F grade. There will be no supplementary examination for such subjects. In the case of failure in the micro-credit subject, the student should repeat the micro-credit subject if offered in the next offered semester. The student is permitted to take alternate micro-credit subject if the subject is not offered in the next semester. (Senate Item 321.A.I.C.1). Grade revision for Micro-Credit Courses is not permitted. (Senate Item 333.C.I.C.1). However, in case the micro-credit is accepted as a subject contributing to the curricular requirement, it will be accounted for in the CGPA calculations. A student is normally not expected to take more than 2 such courses in a semester. The subject may also be a part of minor and micro‐specialization. The students can register for Micro-credit subjects with EAA as backlog (Senate Item 322.H.I.C.6). Minor in a Discipline A department would offer a MINOR in a discipline in which it offers a major. The department would enlist a set of subjects from its curriculum and prescribe a requirement for minor taking six subjects or more (18-24 contact-hours plus 3-9 hours of laboratory) from this set. The subjects would be a combination of mostly core and some electives. Students aspiring for a Minor in a discipline must register for the same in the beginning of the 5th semester. Only those students, who have a CGPA of 7.5 or above, without any Backlog, will be permitted to register for a Minor. An SGPA or CGPA in excess of 8.0 has to be maintained in the subsequent semesters without any Backlog in order to keep the Minor registration active. Should both the SGPA and CGPA fall below 8.0 at any point after registering for the minor; the Minor registration will cease to be active. A student registered for Minor in a discipline must register and pass in all subjects that constitute the requirement for a minor and score a minimum grade point average of 6.00 to be awarded a Minor in that discipline. A student may cover these six subjects as either a depth subject, elective subject or a breadth subject or as additional credits. However, he/she has to take at least three subjects as additional credits to earn a minor. Students enrolled in Dual Degree, Integrated M.Sc. and B.Arch. (Hons.) are permitted to pursue minor in other discipline up to their 10th Semester, while the students of B.Tech.(Hons.) should complete the minor requirements by 8th semester. The students can register for Minor with EAA as backlog (Senate Item 322.H.I.C.6). Micro – Specialization The Institute offers Micro-Specializations to UG students. The salient features are as follows: Each Micro-Specialization has a defined structure in terms of three sequential components: Component-I – One Foundation Course (2-4 credits) that constitutes a Mandatory Requirement and also a Pre-Requisite for subsequent Components. Component-II - One/Two subjects (3-4 credits each) from a Specified list of subjects. Component-III - Project/Design/Term Paper (4 credits) OR one subject (4 credits) from a Specified list. A Student would be required to complete 3-4 subjects (10-14 credits) from the specified list in order to earn a Micro-Specialization. The subjects can be taken through the Breadth/Open Elective component of the curriculum or as Additional Subjects. If specified in the Micro-Specialization curriculum, the subject can also be taken as Micro-Credit/s. A student has to register for a Micro-Specialization. The Registration can be done in the beginning of any Semester beyond first year. In order to register for a Micro-Specialization the student must have completed all curricular requirements upto the previous semester and have a CGPA ≥ 7.0. Thereafter the student must maintain a CGPA or SGPA ≥ 7.5 without any Backlog in the subsequent semesters to keep the Micro-Specialization registration active. GPA for the subjects contributing to the Micro-Specialization will be separately calculated. A minimum GPA of 6.00 is essential to earn the Micro-Specialization. The students can register for Micro-Specialization subjects with EAA as backlog (Senate Item 322.H.I.C.6).',\n",
       "  'metadata': {'source': 'iitkgp_minor_micro.txt'}},\n",
       " {'text': 'Switch Over Switch Over Academic Calendar Academic Units Assistantships Branch Change Certificates and Documents Curricula (UG) Curriculum (PG) FAQ Gymkhana HMC HMC Rules and Regulations Library Facility Minor/Micro PMRF Prize, Medals & Awards Scholarships Student Brotherhood Fund Students Grievance Redressal Committee Switch Over Such students must have completed all the prescribed course work upto 4th Semester (for Financial Engg & Engg Entrepreneurship) /6th semester for AIML in first attempt with a minimum GPA of 6.00. EAA will not be considered for branch change. (Senate, Item 334.H.I.C. Any other item). Application for a Switch Over must be made by intending eligible students when the notification is made during the Spring Semester of the academic year. The students have to accordingly apply online through ERP by the specified deadline. Switchover to Petroleum Engineering Similar to Switchover to Interdisciplinary Dual Degree, students of Dual Degree & 5yr Integrated MSc & 4 yr BS students can switchover to Petroleum Engineering at the end of 2nd year. The students of B Tech in Mechanical Engineering/ Chemical Engineering/ Mining Engineering can switch over to M Tech in Petroleum Engineering with B Tech in parent department. EAA will not be considered for branch change. (Senate, Item 334.H.I.C. Any other item). The students of 5 yr Integrated MSc and 4 yr BS in Geology/Geophysics can switchover to 6 Year Integrated MSc-M Tech Programme (with MTech in Petroleum Engineering) (Senate Item: 314.H.I.C.6) Student can also opt for the switchover at the end of second year in case there is any vacant seat arising out after allocation in 1st year. (Senate Item : 330.H.I.C.10) B.Tech.(Hons.) to Dual Degree Switch Over Third year B.Tech.(Hons.) students will have the option to switch over from 4- year B.Tech.(Hons.) to an existing 5-year Dual Degree course (one-way and not the reverse) in the same department, including a choice of specialization, offered at 2-year M.Tech. level in the Department maintaining proportionate distribution of seats. The student must have completed all the curricular requirements upto sixth semester and must not have any backlog subjects with a minimum CGPA of 6.00. The notification for Switch Over will be issued during the Spring Semester of each academic year. The students have to accordingly (i) apply online and (ii) submit signed hardcopy to Academic Section by the specified deadline. Interdisciplinary Switchover Such students must have completed all the prescribed course work upto 4th Semester (for Financial Engg & Engg Entrepreneurship) /6th semester for AIML in first attempt with a minimum GPA of 6.00. EAA will not be considered for branch change. (Senate, Item 334.H.I.C. Any other item). Application for a Switch Over must be made by intending eligible students when the notification is made during the Spring Semester of the academic year. The students have to accordingly apply online through ERP by the specified deadline. Switchover to Petroleum Engineering Similar to Switchover to Interdisciplinary Dual Degree, students of Dual Degree & 5yr Integrated MSc & 4 yr BS students can switchover to Petroleum Engineering at the end of 2nd year. The students of B Tech in Mechanical Engineering/ Chemical Engineering/ Mining Engineering can switch over to M Tech in Petroleum Engineering with B Tech in parent department. EAA will not be considered for branch change. (Senate, Item 334.H.I.C. Any other item). The students of 5 yr Integrated MSc and 4 yr BS in Geology/Geophysics can switchover to 6 Year Integrated MSc-M Tech Programme (with MTech in Petroleum Engineering) (Senate Item: 314.H.I.C.6) Student can also opt for the switchover at the end of second year in case there is any vacant seat arising out after allocation in 1st year. (Senate Item : 330.H.I.C.10)',\n",
       "  'metadata': {'source': 'iitkgp_switchover.txt'}}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simpler chunking strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'From the **third semester onwards**, undergraduate students, including 2-year MSc students, have the opportunity to opt for additional subjects, contingent on the established rules and regulations [1].', 'metadata': {'question': 'What is an additional subject, and what are the rules regarding an additional subject?'}}\n",
      "{'text': 'Additional subjects are designed to allow students to explore areas of interest and acquire knowledge that may be beneficial for their future careers [1].', 'metadata': {'question': 'What is an additional subject, and what are the rules regarding an additional subject?'}}\n",
      "{'text': 'To be eligible for additional subjects, a student must maintain a **Cumulative Grade Point Average (CGPA) of 7.50 or greater, without any backlogs** [1].', 'metadata': {'question': 'What is an additional subject, and what are the rules regarding an additional subject?'}}\n",
      "{'text': 'Students are allowed to register for additional subjects even with **EAA (Essential Academic Activities) as a backlog** [2].', 'metadata': {'question': 'What is an additional subject, and what are the rules regarding an additional subject?'}}\n",
      "{'text': 'The number of additional credits a student can earn is capped at **33% of the credit requirements for their major** [2].', 'metadata': {'question': 'What is an additional subject, and what are the rules regarding an additional subject?'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk  # For sentence tokenization\n",
    "\n",
    "nltk.download('punkt_tab')  # Download sentence tokenizer data\n",
    "\n",
    "def topic_and_sentence_chunking(processed_data):\n",
    "    \"\"\"Chunks text in processed_data based on topics and sentences.\"\"\"\n",
    "    chunked_data = []\n",
    "    for entry in processed_data:\n",
    "        text = entry[\"text\"]\n",
    "        metadata = entry[\"metadata\"].copy()  # Create a copy to avoid modifying the original\n",
    "\n",
    "        if \"source\" in metadata and metadata[\"source\"].endswith(\".txt\"):  # Apply topic chunking to .txt files\n",
    "            lines = text.split('\\n')\n",
    "            current_topic = None\n",
    "            current_topic_text = \"\"\n",
    "\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue  # Skip empty lines\n",
    "\n",
    "                if line.endswith(':'):\n",
    "                    if current_topic:\n",
    "                        sentences = nltk.sent_tokenize(current_topic_text)\n",
    "                        for sentence in sentences:\n",
    "                            if sentence.strip():\n",
    "                                chunked_data.append({\n",
    "                                    \"text\": sentence.strip(),\n",
    "                                    \"metadata\": {\"topic\": current_topic, **metadata} #merge metadata\n",
    "                                })\n",
    "                        current_topic_text = \"\"\n",
    "                    current_topic = line[:-1].strip()\n",
    "                else:\n",
    "                    current_topic_text += \" \" + line\n",
    "\n",
    "            if current_topic and current_topic_text.strip():\n",
    "                sentences = nltk.sent_tokenize(current_topic_text)\n",
    "                for sentence in sentences:\n",
    "                    if sentence.strip():\n",
    "                        chunked_data.append({\n",
    "                            \"text\": sentence.strip(),\n",
    "                            \"metadata\": {\"topic\": current_topic, **metadata} #merge metadata\n",
    "                        })\n",
    "\n",
    "        elif \"answer\" in entry[\"text\"] or \"question\" in metadata: #apply sentence chunking to json files\n",
    "          sentences = nltk.sent_tokenize(text)\n",
    "          for sentence in sentences:\n",
    "            if sentence.strip():\n",
    "              chunked_data.append({\n",
    "                \"text\": sentence.strip(),\n",
    "                \"metadata\": metadata\n",
    "              })\n",
    "\n",
    "        else: #keep all other entries as they are\n",
    "            chunked_data.append(entry)\n",
    "\n",
    "    return chunked_data\n",
    "\n",
    "# Assuming processed_data is already populated as in your code\n",
    "processed_chunked_data = topic_and_sentence_chunking(processed_data)\n",
    "\n",
    "# Example: Print the first few chunked items\n",
    "for i in range(min(5, len(processed_chunked_data))):\n",
    "    print(processed_chunked_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid chunking with some improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'From the **third semester onwards**, undergraduate students, including 2-year MSc students, have the opportunity to opt for additional subjects, contingent on the established rules and regulations [1].', 'metadata': {'question': 'What is an additional subject, and what are the rules regarding an additional subject?'}}\n",
      "{'text': 'Additional subjects are designed to allow students to explore areas of interest and acquire knowledge that may be beneficial for their future careers [1].', 'metadata': {'question': 'What is an additional subject, and what are the rules regarding an additional subject?'}}\n",
      "{'text': 'To be eligible for additional subjects, a student must maintain a **Cumulative Grade Point Average (CGPA) of 7.50 or greater, without any backlogs** [1].', 'metadata': {'question': 'What is an additional subject, and what are the rules regarding an additional subject?'}}\n",
      "{'text': 'Students are allowed to register for additional subjects even with **EAA (Essential Academic Activities) as a backlog** [2].', 'metadata': {'question': 'What is an additional subject, and what are the rules regarding an additional subject?'}}\n",
      "{'text': 'The number of additional credits a student can earn is capped at **33% of the credit requirements for their major** [2].', 'metadata': {'question': 'What is an additional subject, and what are the rules regarding an additional subject?'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk  # For sentence tokenization\n",
    "\n",
    "nltk.download('punkt_tab')  # Download sentence tokenizer data\n",
    "\n",
    "def topic_and_sentence_chunking(processed_data):\n",
    "    \"\"\"Chunks text in processed_data based on topics and sentences.\"\"\"\n",
    "    chunked_data = []\n",
    "    for entry in processed_data:\n",
    "        text = entry[\"text\"]\n",
    "        metadata = entry[\"metadata\"].copy()  # Create a copy to avoid modifying the original\n",
    "\n",
    "        if \"source\" in metadata and metadata[\"source\"].endswith(\".txt\"):  # Apply topic chunking to .txt files\n",
    "            lines = text.split('\\n')\n",
    "            current_topic = None\n",
    "            current_topic_text = \"\"\n",
    "\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue  # Skip empty lines\n",
    "\n",
    "                # Detect topic (lines ending with ':' or ALL CAPS headings)\n",
    "                if line.endswith(':') or line.isupper():\n",
    "                    if current_topic:\n",
    "                        # Split topic text into sentences and create chunks\n",
    "                        sentences = nltk.sent_tokenize(current_topic_text)\n",
    "                        for sentence in sentences:\n",
    "                            if sentence.strip():\n",
    "                                chunked_data.append({\n",
    "                                    \"text\": sentence.strip(),\n",
    "                                    \"metadata\": {\"topic\": current_topic, **metadata}  # Merge metadata\n",
    "                                })\n",
    "                        current_topic_text = \"\"\n",
    "                    current_topic = line[:-1].strip() if line.endswith(':') else line.strip()\n",
    "                else:\n",
    "                    current_topic_text += \" \" + line\n",
    "\n",
    "            # Handle the last topic\n",
    "            if current_topic and current_topic_text.strip():\n",
    "                sentences = nltk.sent_tokenize(current_topic_text)\n",
    "                for sentence in sentences:\n",
    "                    if sentence.strip():\n",
    "                        chunked_data.append({\n",
    "                            \"text\": sentence.strip(),\n",
    "                            \"metadata\": {\"topic\": current_topic, **metadata}  # Merge metadata\n",
    "                        })\n",
    "\n",
    "        elif \"answer\" in entry[\"text\"] or \"question\" in metadata:  # Apply sentence chunking to Q&A content\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            for sentence in sentences:\n",
    "                if sentence.strip():\n",
    "                    # Add question to metadata if available\n",
    "                    new_metadata = metadata.copy()\n",
    "                    if \"question\" in metadata:\n",
    "                        new_metadata[\"question\"] = metadata[\"question\"]\n",
    "                    chunked_data.append({\n",
    "                        \"text\": sentence.strip(),\n",
    "                        \"metadata\": new_metadata\n",
    "                    })\n",
    "\n",
    "        else:  # Keep all other entries as they are\n",
    "            chunked_data.append(entry)\n",
    "\n",
    "    return chunked_data\n",
    "\n",
    "# Example usage\n",
    "processed_chunked_data = topic_and_sentence_chunking(processed_data)\n",
    "\n",
    "# Print the first few chunked items\n",
    "for i in range(min(5, len(processed_chunked_data))):\n",
    "    print(processed_chunked_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to embed the processed_chunked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\OneDrive\\Documents\\RAG_project\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Asus\\OneDrive\\Documents\\RAG_project\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Asus\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding': [0.009000413119792938, -0.006995189934968948, -0.02166716754436493, -0.03337167575955391, -0.019558876752853394, 0.03985268622636795, 0.037032417953014374, -0.004523812793195248, 0.028543390333652496, 0.0116368243470788, 0.06629471480846405, -0.0013004661304876208, -0.022470910102128983, 0.021916117519140244, 0.03274707496166229, -0.013073545880615711, 0.03575897961854935, -0.04825406149029732, -0.09341385215520859, -0.02244574762880802, -0.021044088527560234, 0.012914509512484074, -0.052560534328222275, -0.038265928626060486, 0.07312196493148804, -0.006599404849112034, 0.002231653081253171, 0.0025817984715104103, -0.030674153938889503, 0.0015197135508060455, 0.03988062962889671, 0.03167644888162613, 0.020262403413653374, -0.004446287639439106, 2.1647169887728523e-06, -0.06071391701698303, -0.05157018080353737, 0.0063461423851549625, -0.043238576501607895, 0.000779717811383307, 0.023992864415049553, 0.0601343959569931, 0.05596589669585228, 0.006336602382361889, -0.020090380683541298, 0.09890281409025192, 0.02234046719968319, -0.025096377357840538, -0.04111451283097267, 0.009673090651631355, -0.002457102993503213, 0.0040581063367426395, -0.06601573526859283, -0.06417760252952576, 0.03622548654675484, -0.025194676592946053, 0.01844400353729725, -0.02082982286810875, 0.07333458960056305, 0.07467355579137802, -0.008238229900598526, -0.019097140058875084, -0.04877940192818642, 0.010112076066434383, 0.021042410284280777, 0.04779324680566788, 0.013839072547852993, -0.0004848994722124189, -0.01955302804708481, -0.028635261580348015, 0.07156930863857269, 0.029746437445282936, 0.022147636860609055, 0.007856962271034718, -0.005391606595367193, -0.014527234248816967, -0.046149492263793945, -0.029235633090138435, -0.02268800511956215, 0.0024814927019178867, 0.061581552028656006, 0.005231745541095734, -0.02649136818945408, 0.09953629970550537, 0.00931356567889452, 0.028703764081001282, 0.008594505488872528, -0.01449130941182375, 0.08240339159965515, 0.018191896378993988, 0.0128631005063653, 0.0401514433324337, -0.014508984982967377, 0.033169373869895935, -0.015401001088321209, -0.081548772752285, 0.04204094782471657, 0.022982634603977203, 0.023000309243798256, 0.01430927962064743, -0.006970453076064587, 0.02412082441151142, -0.033228058367967606, 0.03880638629198074, 0.044100355356931686, -0.02486659586429596, -0.03275739401578903, 0.05385814607143402, -0.03212397173047066, 0.020854108035564423, 0.015593352727591991, 0.040777161717414856, -0.0033817579969763756, 0.0051454161293804646, -0.08832649886608124, 0.0658932775259018, -0.027246957644820213, 0.014604149386286736, 0.008167545311152935, -0.01774609088897705, 0.001344673102721572, -0.037320706993341446, -0.08732308447360992, 0.0058977920562028885, 0.00626050028949976, 0.019055519253015518, 0.012403447180986404, 0.04090769588947296, 0.034593623131513596, 0.04677360877394676, 0.0013163642724975944, -0.03433101251721382, 0.008775674737989902, -0.01681257225573063, 0.02291662059724331, 0.021583303809165955, -0.04270179569721222, 0.0603986531496048, 0.03953889384865761, 0.009569510817527771, -0.03215089440345764, -0.012856980785727501, 0.06705372035503387, 0.027103645727038383, -0.008200909942388535, 0.001869885018095374, 0.031894147396087646, 0.04943336546421051, -0.018150312826037407, 0.014235874637961388, -0.00513888755813241, -0.01608440838754177, -0.0070427339524030685, -0.06878706812858582, -0.006921600550413132, 0.07490938156843185, 0.06533575057983398, -0.0010438713943585753, -0.055470023304224014, 0.025363171473145485, -0.0018233249429613352, -0.016750518232584, 0.033297099173069, -0.018730953335762024, -0.03637703135609627, 0.0044478001073002815, 0.02623729035258293, -0.03586925193667412, -0.013049382716417313, -0.011743784882128239, -0.0374852679669857, -0.01983390748500824, -0.017499879002571106, -0.010978247970342636, -0.05565300211310387, -0.0017050388269126415, -0.023945698514580727, -0.03663517162203789, -0.033947914838790894, -0.04341316595673561, -0.023197511211037636, 0.035999517887830734, 0.03377075493335724, 0.059249646961688995, 0.02806146815419197, 0.015154125168919563, -0.06326517462730408, 0.004761188291013241, 0.0006731727044098079, -0.025971997529268265, 0.024090757593512535, 0.007312898989766836, -0.004397603217512369, 0.0030335737392306328, -0.012279518879950047, -0.028331333771348, 0.046515144407749176, 0.00526941055431962, 0.03455772250890732, -0.05486785247921944, -0.029152654111385345, -0.010759152472019196, 0.008279018104076385, 0.0657244473695755, 0.006417051423341036, -0.04928294196724892, -0.021827783435583115, -0.045610133558511734, 0.015668639913201332, 0.10834585875272751, 0.07449641823768616, 0.00923202559351921, 0.10338129103183746, -0.013095764443278313, -0.004556742962449789, -0.022340252995491028, -0.0001020111158140935, -0.029692625626921654, -0.04787109047174454, -0.04290192574262619, -0.05270646512508392, 0.05535487085580826, -0.033813100308179855, 0.001989132259041071, -0.04816536232829094, 0.016085216775536537, 0.02655063197016716, -0.006109120324254036, 0.0021723557729274035, -6.916494021425024e-05, 0.03646356984972954, -0.011299214325845242, 0.0006408764165826142, -0.03243830427527428, 0.04729008302092552, -0.036687932908535004, 0.010687639936804771, 0.005930813029408455, 0.02744552493095398, 0.01636371947824955, -0.013543701730668545, -0.032318878918886185, -0.014683254063129425, -0.005481530912220478, 0.01628023572266102, -0.006992360111325979, 0.011526335030794144, 0.015178600326180458, -0.03286517411470413, 0.0025104351807385683, 0.06827953457832336, 0.01022852398455143, -0.06497269868850708, -0.02002091147005558, 0.02602686919271946, 0.050573743879795074, -0.11915797740221024, 0.010899621993303299, 0.01019824855029583, 0.004625601228326559, 0.0010753144742920995, -0.012845301069319248, 0.0026865913532674313, 0.0011230582604184747, -0.006165045779198408, -0.02940019965171814, -0.003302040509879589, -0.006772253196686506, -0.027583373710513115, 0.026068663224577904, -0.013864458538591862, 0.010643473826348782, 0.009260371327400208, 0.004846219904720783, 0.002578090876340866, 0.010321677662432194, -0.01865740306675434, -0.002775725442916155, -0.036139342933893204, 0.0055004870519042015, -0.0014997392427176237, 0.09531442075967789, 0.0010425274958834052, -0.03451767936348915, -0.015030622482299805, -0.022718684747815132, 0.0005806208355352283, -0.002911983523517847, 0.044372305274009705, 0.01188017800450325, 0.015191575512290001, -0.031076276674866676, -0.026429930701851845, -0.014892317354679108, -0.050608858466148376, -0.030666513368487358, 0.018373560160398483, 0.05829376354813576, -0.028401032090187073, 0.027571149170398712, 0.05039539933204651, -0.0008513143402524292, 0.002075391123071313, -0.0034352093935012817, 0.005982542410492897, -0.021692492067813873, -0.04561381787061691, 0.020800940692424774, -0.06511037051677704, -0.07123821973800659, -0.053957968950271606, 0.002740434603765607, 0.057432204484939575, -0.012805335223674774, 0.008959373459219933, -0.03022599034011364, -0.0007201604312285781, 0.008961552754044533, -0.03451886773109436, 0.0523962639272213, 0.02232895977795124, -0.012411477975547314, 0.0246573518961668, -0.020290544256567955, -0.05340828001499176, -0.09997495263814926, -0.0736779123544693, -0.001914795720949769, 0.014375679194927216, -0.061543602496385574, -0.009779324755072594, -0.01838679052889347, 0.021512309089303017, -0.0506494864821434, -0.03825344890356064, -0.03635428473353386, -0.048604708164930344, 0.010560817085206509, 0.007809355389326811, -0.014855049550533295, 6.879212014609948e-05, 0.008641835302114487, -0.0011124287266284227, 0.009295675903558731, 0.0356174111366272, 0.014295140281319618, -0.05239437520503998, 0.012344256974756718, -0.013731132261455059, -0.027984553948044777, 0.024926798418164253, 0.009358424693346024, -0.025633180513978004, 0.007434309460222721, -0.009279988706111908, -0.036794908344745636, 0.010098454542458057, 0.06228248029947281, -0.030553124845027924, -0.057918649166822433, 0.032274600118398666, -0.004154738504439592, -0.010593910701572895, -0.02023450843989849, 0.08170736581087112, 0.060223426669836044, 0.03930031135678291, 0.004750493913888931, -0.0007408774690702558, 0.0021172219421714544, -0.031093589961528778, -0.03136123716831207, -0.022514425218105316, 0.00780215859413147, -0.05479196831583977, 0.017009034752845764, 0.0034772458020597696, -0.017629001289606094, -0.011881565675139427, 0.019204746931791306, 0.03464220464229584, -0.017919251695275307, 0.08768764138221741, -0.021431801840662956, 0.05200605466961861, -0.07326458394527435, 0.00978906825184822, -0.002830995013937354, 0.06940958648920059, 0.04200093075633049, 0.019201576709747314, -0.11105199158191681, 0.0015728946309536695, 0.02708512358367443, -0.004834608640521765, -0.0013801242457702756, -0.02643507719039917, -0.01209205575287342, 0.03268994390964508, 0.012020065449178219, 0.05161160230636597, -0.014578944072127342, -0.019625291228294373, 0.0006404133164323866, 0.029962919652462006, -0.01630786992609501, -0.022879615426063538, -0.03462284058332443, -0.08292533457279205, -0.022395804524421692, 0.06429386138916016, -0.030772684141993523, -0.03067018836736679, -0.02551206387579441, 0.031511638313531876, 0.004547105636447668, -0.006211570464074612, -0.03028218448162079, 0.02491059899330139, 0.00823097862303257, -0.024150660261511803, -0.002913249423727393, 0.1023092269897461, -0.04568261653184891, -0.027722125872969627, -0.00815658550709486, 0.041084323078393936, -0.008946799673140049, -0.003984600305557251, -0.016883837059140205, -0.022140007466077805, 0.08858213573694229, -0.04647195711731911, 0.013531265780329704, -0.057315316051244736, 0.01967380754649639, 0.030239304527640343, -0.017250269651412964, -0.07455907016992569, 0.061936791986227036, -0.039434436708688736, 0.05143790692090988, 0.06665533035993576, -0.006592012941837311, 0.10793130844831467, 0.02525532990694046, -0.020572153851389885, -0.06519662588834763, 0.014375614933669567, 0.013289980590343475, 0.017016448080539703, 0.021372653543949127, -0.037962932139635086, -0.026463152840733528, 0.01448247954249382, -0.07579486072063446, -0.08168074488639832, 0.02115413174033165, -0.015541893430054188, -0.005944913253188133, 0.06117892637848854, 0.03309621289372444, 0.004211451858282089, 0.028905468061566353, 0.005435314029455185, -0.08770632743835449, 0.009538016282022, 0.0633443146944046, 0.041775137186050415, 0.011504528112709522, 0.009918739087879658, -0.014034116640686989, -0.05178283900022507, 0.027370387688279152, -0.020904093980789185, 0.005246814806014299, -0.0013670974876731634, -0.004440405406057835, -0.0047325249761343, 0.034660328179597855, 0.05742667615413666, 0.041592832654714584, 0.0024548755027353764, 0.023329317569732666, 0.004875081591308117, 0.0393303707242012, 0.03637014329433441, -0.017269376665353775, 8.449541383015458e-06, -0.09172683209180832, 0.0464031957089901, -0.0021931915543973446, -0.04463980346918106, -0.04677806794643402, -0.007868662476539612, -0.013184115290641785, -0.01328242477029562, 0.07519710063934326, -0.014937368221580982, 0.03684186935424805, 0.035896141082048416, 0.020956775173544884, -0.03628705441951752, -0.008663803339004517, -0.0340043343603611, -0.010666893795132637, -0.04477836191654205, 0.043983783572912216, 0.004976039286702871, -0.005336642265319824, 0.03787079080939293, -0.0238536074757576, -0.020775966346263885, -0.03226757049560547, 0.020058097317814827, 0.04055133834481239, -0.05703550577163696, -0.010606547817587852, 0.01415979489684105, 0.0005803750245831907, 0.04526927322149277, -0.08003216981887817, 0.0660523846745491, -0.04308212921023369, -0.03716469556093216, -0.0499640591442585, 0.016605226323008537, -0.006041855551302433, -0.014337308704853058, 0.010440611280500889, -0.024911953136324883, -0.02825191430747509, 0.01146720815449953, 0.03293543681502342, 0.003896596608683467, -0.048135798424482346, 0.00016162001702468842, 0.04605346545577049, 0.025717442855238914, 0.07521186769008636, -0.04327258840203285, -0.03361994028091431, 0.011025243438780308, -0.01247482281178236, 0.007869922555983067, -0.04082963243126869, 0.007325748447328806, 0.0033268826082348824, -0.011774764396250248, -0.021264757961034775, 0.0024780204985290766, 0.05077141523361206, 0.06308016926050186, 0.018815435469150543, 0.04780501499772072, -6.306658700592071e-33, -0.0022931748535484076, 0.03366294875741005, 0.016026243567466736, -0.002817880129441619, -0.02967224456369877, -0.024250993505120277, 0.0403781421482563, -0.02423284761607647, -0.02097671665251255, -0.03652367740869522, -0.007664295844733715, 0.030630920082330704, 0.00532584497705102, 0.015102127566933632, 0.07590814679861069, 0.028426634147763252, -0.005186017602682114, 0.02645397186279297, 0.017367400228977203, -0.010270997881889343, -0.03165717050433159, 0.011832313612103462, 0.04789124056696892, 0.003209490329027176, -0.00984166469424963, 0.014844409190118313, 0.04565935954451561, -0.041962433606386185, -0.028460150584578514, 0.008376654237508774, 0.04347483068704605, -0.007927563972771168, 0.03150106221437454, 0.009680251590907574, 0.01261933520436287, -0.006660748738795519, -0.03404209762811661, -0.02783726155757904, 0.00430243881419301, 0.0031816258560866117, 0.02542257308959961, -0.08378725498914719, 0.016553664579987526, -0.022791484370827675, -0.010959507897496223, -0.05203758925199509, -0.03251780569553375, 0.01009272038936615, 0.004105146508663893, -0.0030815370846539736, -0.04401347413659096, -0.03418077155947685, -0.014224661514163017, 0.07789123803377151, -0.016129378229379654, 0.009203033521771431, 0.018807303160429, -0.0305661391466856, 0.09885044395923615, 0.06467748433351517, -0.020475439727306366, 0.041321154683828354, -0.019400814548134804, -0.05174926668405533, -0.09053288400173187, -0.03243611380457878, -0.054184142500162125, 0.00609453534707427, 0.02930392138659954, -0.014895734377205372, -0.021386906504631042, 0.08149738609790802, -0.0013910449342802167, -0.012483007274568081, 0.005068677943199873, -0.019959241151809692, -0.05037452653050423, -0.007901777513325214, 0.0601465106010437, 0.04160216078162193, -0.04635186865925789, 0.011210709810256958, -0.007477070204913616, -0.03131777420639992, -0.009181097149848938, -0.002472067717462778, -0.01367007102817297, 0.04137822613120079, 0.02515799179673195, -0.05471391603350639, 0.012375622987747192, 0.026902152225375175, 0.01439089048653841, -0.01817670837044716, 0.0007209298782981932, -0.042971063405275345, 0.032548174262046814, 0.0012055323459208012, -0.0009407131001353264, 0.018889175727963448, -0.04640759527683258, 0.0017431618180125952, 0.03607391193509102, -0.024929752573370934, 0.03103034198284149, 0.032034147530794144, -0.008183573372662067, -0.033771585673093796, -0.004157921299338341, -0.004107183311134577, -0.011010962538421154, -0.05441577732563019, 0.030161816626787186, 0.04080173745751381, 0.024657992646098137, 0.034225814044475555, -0.015237230807542801, -0.06475932151079178, -0.005826124455779791, 0.012962658889591694, -0.03724425286054611, -0.08701099455356598, 0.0030216374434530735, 0.021277254447340965, -0.0034374527167528868, 0.024271784350275993, 0.004518418572843075, -0.027669154107570648, 0.016215842217206955, -0.05475525185465813, 0.026053225621581078, -0.02587791532278061, 2.8392992135195527e-07, 0.0006751849432475865, 0.06599318981170654, -0.011881843209266663, -0.0442914143204689, -0.029908018186688423, -0.07111503183841705, -0.04996541514992714, 0.03513849899172783, -0.01766871102154255, 0.07591159641742706, 0.006010841578245163, -0.029575180262327194, 0.036650754511356354, -0.05369175225496292, 0.035904958844184875, -0.07003085315227509, 0.00793648511171341, -0.0008412074530497193, -0.0301178228110075, 0.00653483159840107, -0.0019309918861836195, 0.028162898495793343, 0.020974257960915565, -0.02524692937731743, -0.01217069011181593, -0.008321243338286877, -0.00012722644896712154, -0.01719660498201847, 0.033672478049993515, 0.04139330983161926, 0.055414680391550064, 0.02646913379430771, 0.056462522596120834, 0.030365576967597008, 0.0033423397690057755, -0.06033320724964142, 0.03688852861523628, 0.03416949883103371, 0.02857237495481968, 0.017855793237686157, -0.017353011295199394, 0.018065454438328743, -0.02560211904346943, 0.02245156839489937, 0.024717604741454124, 0.033593326807022095, -0.001425482681952417, 0.007415763568133116, -0.09376063197851181, -0.0013838394079357386, 0.030263356864452362, 0.029615044593811035, -0.006523509509861469, -0.03514741733670235, 0.017559131607413292, 0.006298965774476528, -0.0006079044542275369, 0.03216158598661423, 0.03988156095147133, 0.014967689290642738, 0.0029684919863939285, -0.08673021197319031, -0.005270208232104778, -0.01888333633542061, 0.09750314801931381, -0.05276923254132271, -0.03781532868742943, 2.6202494580248437e-34, 0.04452664405107498, -0.008059591986238956, 0.03165484219789505, 0.044081754982471466, -0.05429524555802345, -0.02219800464808941, -0.021743187680840492, 0.026970012113451958, 0.0075975037179887295, -0.011804522946476936, -0.03989068418741226], 'text': 'From the **third semester onwards**, undergraduate students, including 2-year MSc students, have the opportunity to opt for additional subjects, contingent on the established rules and regulations [1].', 'metadata': {'question': 'What is an additional subject, and what are the rules regarding an additional subject?'}}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "model = SentenceTransformer('all-mpnet-base-v2') #a good general purpose model.\n",
    "\n",
    "def embed_chunks(chunked_data):\n",
    "    \"\"\"Embeds the text in chunked_data using Sentence Transformers.\"\"\"\n",
    "    embedded_data = []\n",
    "    for entry in chunked_data:\n",
    "        embedding = model.encode(entry[\"text\"])\n",
    "        embedded_data.append({\n",
    "            \"embedding\": embedding.tolist(),  # Convert numpy array to list for JSON serialization\n",
    "            \"text\": entry[\"text\"],\n",
    "            \"metadata\": entry[\"metadata\"]\n",
    "        })\n",
    "    return embedded_data\n",
    "\n",
    "# Assuming processed_chunked_data is already populated\n",
    "embedded_data = embed_chunks(processed_chunked_data)\n",
    "\n",
    "# Example: Print the first embedded item\n",
    "print(embedded_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For deleting the chromadb instance and vectorbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'iitkgp_data' deleted.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./vectorized_db\")\n",
    "client.delete_collection(name=\"iitkgp_data\")\n",
    "print(\"Collection 'iitkgp_data' deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for creating and saving vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB collection created, populated, and saved.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import uuid #import uuid\n",
    "\n",
    "# Initialize ChromaDB client with persistence\n",
    "client = chromadb.PersistentClient(path=\"./vectorized_db\")\n",
    "\n",
    "# Create Embedding Function\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "# Create a collection with persistence\n",
    "collection = client.create_collection(name=\"iitkgp_data\", embedding_function=sentence_transformer_ef)\n",
    "\n",
    "# Assuming 'embedded_data' is your list of dictionaries with embeddings, text, and metadata\n",
    "for entry in embedded_data:\n",
    "    collection.add(\n",
    "        embeddings=[entry[\"embedding\"]],\n",
    "        documents=[entry[\"text\"]],\n",
    "        metadatas=[entry[\"metadata\"]],\n",
    "        ids=[str(uuid.uuid4())] #add ids\n",
    "    )\n",
    "\n",
    "print(\"ChromaDB collection created, populated, and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings # using hugging face embeddings to match chromadb\n",
    "\n",
    "# Initialize ChromaDB client with persistence\n",
    "client = chromadb.PersistentClient(path=\"./vectorized_db\")\n",
    "\n",
    "# Create Embedding Function\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "# Load the collection\n",
    "collection = client.get_collection(name=\"iitkgp_data\", embedding_function=sentence_transformer_ef)\n",
    "\n",
    "# Create LangChain embeddings object (using HuggingFaceEmbeddings to match chromadb embeddings)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "# Create LangChain Chroma vectorstore\n",
    "vectorstore = Chroma(client=client, collection_name=\"iitkgp_data\", embedding_function=embeddings)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how to determine the rating of Cafe coffee day based on the given context. Let me read through the context again carefully.\n",
      "\n",
      "The context mentions that Micro-Credit courses can be held during evenings, weekdays, or weekends. It also talks about Micro-Credit courses being one-credit courses lasting 3-14 days and offered during the semester or in available free slots. The grade is indicated under semester 9.\n",
      "\n",
      "Then there's a section about students admitted to the 4-year B.Tech.(Hons.) program, which states that they must have a minimum CGPA of 7.0 without any backlog at the time of application.\n",
      "\n",
      "The question is asking for the rating of Cafe coffee day. Hmm, I don't see any mention of cafes or coffee days in this context. The focus seems to be on Micro-Credit courses and B.Tech students' academic requirements.\n",
      "\n",
      "Wait, maybe there's a typo or something missing. Perhaps it's referring to a specific institution that provides these services, but the provided context doesn't include information about Cafe coffee day ratings.\n",
      "\n",
      "Since I don't have any data related to cafes or their service quality in this context, I can't provide an accurate rating for Cafe coffee day. It might be necessary to ask more details about the institution or the specific cafe they're referring to.\n",
      "</think>\n",
      "\n",
      "The provided context does not include information about Cafe coffee day ratings. More details about cafes and their services would be needed to determine their rating.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Initialize LLM (replace with your chosen LLM)\n",
    "#you will need to have your openai key set as an environment variable\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "\n",
    "Here is the context to use to answer the question:\n",
    "\n",
    "{context} \n",
    "\n",
    "Think carefully about the above context. \n",
    "\n",
    "Now, review the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Provide an answer to this questions using only the above context. \n",
    "\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "question = \"What is the rating of Cafe coffee day?\"\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "print(generation.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever Contents:\n",
      "\n",
      "Document 1:\n",
      "  Page Content: They can be held during evenings, weekdays, or weekends.\n",
      "  Metadata: {'category': 'micro_credits', 'question': 'What are Micro-Credit courses?'}\n",
      "----------------------------------------\n",
      "Document 2:\n",
      "  Page Content: Micro-Credit courses are one-credit courses lasting 3-14 days, offered during the semester or in available free slots.\n",
      "  Metadata: {'category': 'micro_credits', 'question': 'What are Micro-Credit courses?'}\n",
      "----------------------------------------\n",
      "Document 3:\n",
      "  Page Content: The grade will be indicated in the transcript under semester 9.\n",
      "  Metadata: {'category': 'semester_away_project_program_sapp', 'question': 'How is the final evaluation conducted for SAPP?'}\n",
      "----------------------------------------\n",
      "Document 4:\n",
      "  Page Content: Students admitted to the 4-year B.Tech.(Hons.)\n",
      "  Metadata: {'category': 'switchover_dual_degree', 'question': 'What are the options for switching over to Dual Degree programs?'}\n",
      "----------------------------------------\n",
      "Document 5:\n",
      "  Page Content: - A minimum CGPA of 7.0 without any backlog at the time of application.\n",
      "  Metadata: {'category': 'double_major', 'question': 'What are the eligibility criteria for a Double Major?'}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def display_retriever_contents(retriever, query):\n",
    "    \"\"\"\n",
    "    Displays the contents that the retriever extracts from the vector database.\n",
    "\n",
    "    Args:\n",
    "        retriever: The LangChain retriever object.\n",
    "        query: The query string to use for retrieval.\n",
    "    \"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    if not docs:\n",
    "        print(\"Retriever found no relevant documents.\")\n",
    "        return\n",
    "\n",
    "    print(\"Retriever Contents:\\n\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Document {i + 1}:\")\n",
    "        print(f\"  Page Content: {doc.page_content}\")\n",
    "        print(f\"  Metadata: {doc.metadata}\")\n",
    "        print(\"-\" * 40)\n",
    "question = \"What is the rating of Cafe coffee day?\"\n",
    "# Example Usage (assuming 'retriever' and 'question' are already defined)\n",
    "display_retriever_contents(retriever, question) #use the same question variable from your other code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
